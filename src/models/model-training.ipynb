{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13100235,"sourceType":"datasetVersion","datasetId":8298257},{"sourceId":13107418,"sourceType":"datasetVersion","datasetId":8302896}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install timm==1.0.9 transformers==4.44.2 accelerate==0.34.2 --upgrade\n!pip -q install openai-whisper==20231117 transformers==4.44.2 timm==1.0.9 accelerate==0.34.2 soundfile==0.12.1 --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T15:56:24.912612Z","iopub.execute_input":"2025-09-20T15:56:24.913464Z","iopub.status.idle":"2025-09-20T16:01:12.253104Z","shell.execute_reply.started":"2025-09-20T15:56:24.913436Z","shell.execute_reply":"2025-09-20T16:01:12.252039Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.2/779.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport json\n\n# Processed manifest from your preprocessing notebook\nPROC_DIR = Path(\"/kaggle/input/thesis-data-processed\")\nMANIFEST = PROC_DIR / \"hatemm_train_manifest.csv\"  # created previously\nassert MANIFEST.exists(), \"Missing hatemm_train_manifest.csv. Run the preprocessing notebook first.\"\n\n# Use the existing cache_hatemm layout you already created\nCACHE_DIR = Path(\"/kaggle/input/thesis-data-processed\")\nV_FEAT_DIR = CACHE_DIR / \"vision\"   # existing .npy features (vision-generic)\nA_FEAT_DIR = CACHE_DIR / \"audio\"    # existing .npy features (audio-generic)\n\nprint(\"Manifest:\", MANIFEST)\nprint(\"Vision dir exists:\", V_FEAT_DIR.exists(), V_FEAT_DIR)\nprint(\"Audio dir exists:\", A_FEAT_DIR.exists(), A_FEAT_DIR)\n\ndf = pd.read_csv(MANIFEST)\nprint(\"Loaded manifest rows:\", len(df))\ndf.head(2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T16:01:12.255172Z","iopub.execute_input":"2025-09-20T16:01:12.255843Z","iopub.status.idle":"2025-09-20T16:01:12.918851Z","shell.execute_reply.started":"2025-09-20T16:01:12.255803Z","shell.execute_reply":"2025-09-20T16:01:12.918003Z"}},"outputs":[{"name":"stdout","text":"Manifest: /kaggle/input/thesis-data-processed/hatemm_train_manifest.csv\nVision dir exists: False /kaggle/input/thesis-data-processed/vision\nAudio dir exists: False /kaggle/input/thesis-data-processed/audio\nLoaded manifest rows: 1083\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"  dataset          video_id  \\\n0  HateMM  hate_video_1.mp4   \n1  HateMM  hate_video_2.mp4   \n\n                                          video_path  start_sec  end_sec  \\\n0  /kaggle/input/thesis-dataset/Thesis_dataset/ha...        NaN      NaN   \n1  /kaggle/input/thesis-dataset/Thesis_dataset/ha...        NaN      NaN   \n\n  target_group    label  binary_offensive  \\\n0   ['Blacks']  hateful                 1   \n1   ['Blacks']  hateful                 1   \n\n                                  sample_id  video_seconds  segment_seconds  \\\n0  HateMM__hate_video_1.mp4__0_end__hateful         94.998              NaN   \n1  HateMM__hate_video_2.mp4__0_end__hateful        129.160              NaN   \n\n   is_broken                                   vision_feat_path  \\\n0      False  /kaggle/working/cache_hatemm/vision/HateMM__ha...   \n1      False  /kaggle/working/cache_hatemm/vision/HateMM__ha...   \n\n                                     audio_feat_path  has_vision  has_audio  \n0  /kaggle/working/cache_hatemm/audio/HateMM__hat...        True       True  \n1  /kaggle/working/cache_hatemm/audio/HateMM__hat...        True       True  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>video_id</th>\n      <th>video_path</th>\n      <th>start_sec</th>\n      <th>end_sec</th>\n      <th>target_group</th>\n      <th>label</th>\n      <th>binary_offensive</th>\n      <th>sample_id</th>\n      <th>video_seconds</th>\n      <th>segment_seconds</th>\n      <th>is_broken</th>\n      <th>vision_feat_path</th>\n      <th>audio_feat_path</th>\n      <th>has_vision</th>\n      <th>has_audio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HateMM</td>\n      <td>hate_video_1.mp4</td>\n      <td>/kaggle/input/thesis-dataset/Thesis_dataset/ha...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>['Blacks']</td>\n      <td>hateful</td>\n      <td>1</td>\n      <td>HateMM__hate_video_1.mp4__0_end__hateful</td>\n      <td>94.998</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>/kaggle/working/cache_hatemm/vision/HateMM__ha...</td>\n      <td>/kaggle/working/cache_hatemm/audio/HateMM__hat...</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HateMM</td>\n      <td>hate_video_2.mp4</td>\n      <td>/kaggle/input/thesis-dataset/Thesis_dataset/ha...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>['Blacks']</td>\n      <td>hateful</td>\n      <td>1</td>\n      <td>HateMM__hate_video_2.mp4__0_end__hateful</td>\n      <td>129.160</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>/kaggle/working/cache_hatemm/vision/HateMM__ha...</td>\n      <td>/kaggle/working/cache_hatemm/audio/HateMM__hat...</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# === Rewrite feature paths in manifest to use a portable CACHE_DIR (INPUT is read-only) ===\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\n\n# ---- CONFIG (chỉnh 2 dòng này) ----\nPROC_DIR    = Path(\"/kaggle/input/thesis-data-processed\")               # nơi bạn có quyền ghi\nIN_MANIFEST = PROC_DIR / \"hatemm_train_manifest.csv\"          # manifest cũ do preprocessing tạo\nCACHE_DIR   = Path(\"/kaggle/input/thesis-data-processed/features\")     # <-- ĐỔI: tên folder bạn đã upload (INPUT - READ ONLY)\n\n# Chỉ đọc từ INPUT; KHÔNG mkdir trong INPUT\nV_FEAT_DIR = CACHE_DIR / \"vision\"\nA_FEAT_DIR = CACHE_DIR / \"audio\"\n\nassert IN_MANIFEST.exists(), f\"Manifest not found: {IN_MANIFEST}\"\ndf = pd.read_csv(IN_MANIFEST)\n\ndef ensure_col(df, col):\n    if col not in df.columns:\n        df[col] = pd.Series([np.nan]*len(df))\n    return df\n\ndf = ensure_col(df, \"sample_id\")\ndf = ensure_col(df, \"vision_feat_path\")\ndf = ensure_col(df, \"audio_feat_path\")\n\ndef rewrite_to_cache_dir(path_value, sample_id: str, subdir: Path):\n    \"\"\"\n    - Nếu path cũ có sẵn -> lấy basename và ghép lại với CACHE_DIR/<subdir>/<basename>.\n    - Nếu path rỗng -> synthesize từ sample_id: CACHE_DIR/<subdir>/<sample_id>.npy\n    \"\"\"\n    try:\n        if isinstance(path_value, str) and path_value.strip():\n            base = Path(path_value).name\n            return str(subdir / base)\n    except Exception:\n        pass\n    if isinstance(sample_id, str) and sample_id:\n        return str(subdir / f\"{sample_id}.npy\")\n    return np.nan\n\ndf[\"vision_feat_path\"] = [\n    rewrite_to_cache_dir(p, sid, V_FEAT_DIR) for p, sid in zip(df[\"vision_feat_path\"], df[\"sample_id\"])\n]\ndf[\"audio_feat_path\"] = [\n    rewrite_to_cache_dir(p, sid, A_FEAT_DIR) for p, sid in zip(df[\"audio_feat_path\"], df[\"sample_id\"])\n]\n\n# (Optional) Kiểm tra tồn tại file trên INPUT\ndef exists_safe(p):\n    try:\n        return Path(p).exists()\n    except Exception:\n        return False\n\nn_v_exist = int(df[\"vision_feat_path\"].apply(exists_safe).sum())\nn_a_exist = int(df[\"audio_feat_path\"].apply(exists_safe).sum())\nprint(f\"Vision features found: {n_v_exist}/{len(df)}\")\nprint(f\"Audio  features found: {n_a_exist}/{len(df)}\")\n\n# ---- SAVE ra /kaggle/working (có quyền ghi) ----\nOUT_PROC_DIR = Path(\"/kaggle/working\")\nOUT_MANIFEST = OUT_PROC_DIR / \"hatemm_train_manifest.rewritten.csv\"\nPROC_DIR.mkdir(parents=True, exist_ok=True)   # working dir -> OK\ndf.to_csv(OUT_MANIFEST, index=False)\nprint(\"Rewritten manifest saved to:\", OUT_MANIFEST)\n\n# Preview\ndf[[\"sample_id\",\"label\",\"vision_feat_path\",\"audio_feat_path\"]].head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T16:01:12.919829Z","iopub.execute_input":"2025-09-20T16:01:12.920546Z","iopub.status.idle":"2025-09-20T16:01:21.488671Z","shell.execute_reply.started":"2025-09-20T16:01:12.920522Z","shell.execute_reply":"2025-09-20T16:01:21.488035Z"}},"outputs":[{"name":"stdout","text":"Vision features found: 1081/1083\nAudio  features found: 1068/1083\nRewritten manifest saved to: /kaggle/working/hatemm_train_manifest.rewritten.csv\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                     sample_id    label  \\\n0     HateMM__hate_video_1.mp4__0_end__hateful  hateful   \n1     HateMM__hate_video_2.mp4__0_end__hateful  hateful   \n2  HateMM__non_hate_video_1.mp4__0_end__normal   normal   \n3     HateMM__hate_video_3.mp4__0_end__hateful  hateful   \n4  HateMM__non_hate_video_2.mp4__0_end__normal   normal   \n\n                                    vision_feat_path  \\\n0  /kaggle/input/thesis-data-processed/features/v...   \n1  /kaggle/input/thesis-data-processed/features/v...   \n2  /kaggle/input/thesis-data-processed/features/v...   \n3  /kaggle/input/thesis-data-processed/features/v...   \n4  /kaggle/input/thesis-data-processed/features/v...   \n\n                                     audio_feat_path  \n0  /kaggle/input/thesis-data-processed/features/a...  \n1  /kaggle/input/thesis-data-processed/features/a...  \n2  /kaggle/input/thesis-data-processed/features/a...  \n3  /kaggle/input/thesis-data-processed/features/a...  \n4  /kaggle/input/thesis-data-processed/features/a...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>label</th>\n      <th>vision_feat_path</th>\n      <th>audio_feat_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HateMM__hate_video_1.mp4__0_end__hateful</td>\n      <td>hateful</td>\n      <td>/kaggle/input/thesis-data-processed/features/v...</td>\n      <td>/kaggle/input/thesis-data-processed/features/a...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HateMM__hate_video_2.mp4__0_end__hateful</td>\n      <td>hateful</td>\n      <td>/kaggle/input/thesis-data-processed/features/v...</td>\n      <td>/kaggle/input/thesis-data-processed/features/a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HateMM__non_hate_video_1.mp4__0_end__normal</td>\n      <td>normal</td>\n      <td>/kaggle/input/thesis-data-processed/features/v...</td>\n      <td>/kaggle/input/thesis-data-processed/features/a...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HateMM__hate_video_3.mp4__0_end__hateful</td>\n      <td>hateful</td>\n      <td>/kaggle/input/thesis-data-processed/features/v...</td>\n      <td>/kaggle/input/thesis-data-processed/features/a...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HateMM__non_hate_video_2.mp4__0_end__normal</td>\n      <td>normal</td>\n      <td>/kaggle/input/thesis-data-processed/features/v...</td>\n      <td>/kaggle/input/thesis-data-processed/features/a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from pathlib import Path\nimport pandas as pd\n\nMANIFEST = Path(\"/kaggle/working/hatemm_train_manifest.rewritten.csv\")\ndf = pd.read_csv(MANIFEST)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T16:01:21.490365Z","iopub.execute_input":"2025-09-20T16:01:21.490606Z","iopub.status.idle":"2025-09-20T16:01:21.506790Z","shell.execute_reply.started":"2025-09-20T16:01:21.490585Z","shell.execute_reply":"2025-09-20T16:01:21.506227Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\n\ndef safe_path(p):\n    return str(p) if (isinstance(p, (str, Path)) and Path(p).exists()) else None\n\n# If your manifest already has paths, trust them; otherwise, try to infer from sample_id\nif \"vision_feat_path\" not in df.columns or \"audio_feat_path\" not in df.columns:\n    # Infer .npy path by sample_id\n    def guess_feat(sample_id, root: Path):\n        cand = root / f\"{sample_id}.npy\"\n        return str(cand) if cand.exists() else None\n\n    df[\"vision_feat_path\"] = df[\"sample_id\"].apply(lambda sid: guess_feat(sid, V_FEAT_DIR))\n    df[\"audio_feat_path\"]  = df[\"sample_id\"].apply(lambda sid: guess_feat(sid, A_FEAT_DIR))\nelse:\n    # Normalize (if stored as relative paths)\n    df[\"vision_feat_path\"] = df[\"vision_feat_path\"].apply(lambda p: safe_path(p))\n    df[\"audio_feat_path\"]  = df[\"audio_feat_path\"].apply(lambda p: safe_path(p))\n\n# Keep rows that have at least one modality available\nusable = df[(df[\"vision_feat_path\"].notna()) | (df[\"audio_feat_path\"].notna())].copy()\nprint(\"Usable rows:\", len(usable), \"/\", len(df))\n\n# Label mapping\nlabels_sorted = sorted(usable[\"label\"].unique())\nlabel2id = {lbl:i for i,lbl in enumerate(labels_sorted)}\nid2label = {i:lbl for lbl,i in label2id.items()}\nprint(\"Classes:\", label2id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T16:01:21.507515Z","iopub.execute_input":"2025-09-20T16:01:21.508036Z","iopub.status.idle":"2025-09-20T16:01:24.451558Z","shell.execute_reply.started":"2025-09-20T16:01:21.508009Z","shell.execute_reply":"2025-09-20T16:01:24.450805Z"}},"outputs":[{"name":"stdout","text":"Usable rows: 1083 / 1083\nClasses: {'hateful': 0, 'normal': 1}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# === Replace Cell A2 with this robust builder ===\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\ndef load_npy_dim(path):\n    try:\n        p = Path(path)\n        if p.exists():\n            arr = np.load(p, mmap_mode=\"r\")\n            if arr.ndim == 1:\n                return int(arr.shape[0])\n            # if accidentally saved as (1,D) etc.\n            return int(np.prod(arr.shape))\n    except Exception:\n        return np.nan\n    return np.nan\n\ndef load_npy(path):\n    try:\n        p = Path(path)\n        if p.exists():\n            a = np.load(p)\n            return a.squeeze().astype(np.float32)\n    except Exception:\n        return None\n    return None\n\n# --- Diagnose feature dimensions per modality ---\nusable = df.copy()\n\nusable[\"v_dim\"] = usable[\"vision_feat_path\"].apply(load_npy_dim) if \"vision_feat_path\" in usable.columns else np.nan\nusable[\"a_dim\"] = usable[\"audio_feat_path\"].apply(load_npy_dim)  if \"audio_feat_path\"  in usable.columns else np.nan\n\n# Count distribution\nv_counts = usable[\"v_dim\"].dropna().astype(int).value_counts().sort_index()\na_counts = usable[\"a_dim\"].dropna().astype(int).value_counts().sort_index()\nprint(\"Vision dim counts:\\n\", v_counts.to_string())\nprint(\"\\nAudio  dim counts:\\n\", a_counts.to_string())\n\n# --- Strategy toggle ---\nUSE_PADDING = False   # False = keep only majority-dim; True = zero-pad to max dim\n\n# Majority dims\nv_major = int(v_counts.idxmax()) if len(v_counts)>0 else None\na_major = int(a_counts.idxmax()) if len(a_counts)>0 else None\nprint(f\"\\nChosen majority dims -> vision: {v_major}, audio: {a_major} (set USE_PADDING=True to pad instead)\")\n\ndef to_dense_pad(vectors, target_dim=None):\n    \"\"\"Pad 1D vectors to same length (max or provided target_dim).\"\"\"\n    if len(vectors) == 0:\n        return np.zeros((0, 0), dtype=np.float32)\n    if target_dim is None:\n        target_dim = max(v.shape[0] for v in vectors)\n    M = np.zeros((len(vectors), target_dim), dtype=np.float32)\n    for i, v in enumerate(vectors):\n        d = min(target_dim, v.shape[0])\n        M[i, :d] = v[:d]\n    return M\n\n# --- Builders for three setups ---\nlabels_sorted = sorted(usable[\"label\"].unique())\nlabel2id = {lbl:i for i,lbl in enumerate(labels_sorted)}\nid2label = {i:lbl for lbl,i in label2id.items()}\nprint(\"Classes:\", label2id)\n\ndef build_V_only(df_in):\n    rows = []\n    for _, r in df_in.iterrows():\n        p = r.get(\"vision_feat_path\")\n        v = load_npy(p) if isinstance(p, str) else None\n        if v is None:\n            continue\n        rows.append((v, label2id[r[\"label\"]], str(r[\"video_id\"])))\n    if not rows:\n        return np.zeros((0,0), np.float32), np.zeros((0,), np.int64), np.array([])\n    vecs, Y, G = zip(*rows)\n    if USE_PADDING:\n        X = to_dense_pad(list(vecs))  # pad to max vision dim\n    else:\n        # keep only majority-dim vectors\n        keep = [v for v in vecs if v.shape[0] == v_major]\n        idxs = [i for i,v in enumerate(vecs) if v.shape[0] == v_major]\n        X = np.stack(keep, axis=0)\n        Y = np.array([Y[i] for i in idxs], dtype=np.int64)\n        G = np.array([G[i] for i in idxs])\n    return X, Y, G\n\ndef build_A_only(df_in):\n    rows = []\n    for _, r in df_in.iterrows():\n        p = r.get(\"audio_feat_path\")\n        a = load_npy(p) if isinstance(p, str) else None\n        if a is None:\n            continue\n        rows.append((a, label2id[r[\"label\"]], str(r[\"video_id\"])))\n    if not rows:\n        return np.zeros((0,0), np.float32), np.zeros((0,), np.int64), np.array([])\n    vecs, Y, G = zip(*rows)\n    if USE_PADDING:\n        X = to_dense_pad(list(vecs))  # pad to max audio dim\n    else:\n        keep = [a for a in vecs if a.shape[0] == a_major]\n        idxs = [i for i,a in enumerate(vecs) if a.shape[0] == a_major]\n        X = np.stack(keep, axis=0)\n        Y = np.array([Y[i] for i in idxs], dtype=np.int64)\n        G = np.array([G[i] for i in idxs])\n    return X, Y, G\n\ndef build_VA_fusion(df_in):\n    rows = []\n    for _, r in df_in.iterrows():\n        pv = r.get(\"vision_feat_path\"); pa = r.get(\"audio_feat_path\")\n        v = load_npy(pv) if isinstance(pv, str) else None\n        a = load_npy(pa) if isinstance(pa, str) else None\n        if v is None or a is None:\n            continue\n        rows.append((v, a, label2id[r[\"label\"]], str(r[\"video_id\"])))\n    if not rows:\n        return np.zeros((0,0), np.float32), np.zeros((0,), np.int64), np.array([])\n    V, A, Y, G = zip(*rows)\n\n    if USE_PADDING:\n        Vd = to_dense_pad(list(V))   # pad each modality separately\n        Ad = to_dense_pad(list(A))\n        X  = np.concatenate([Vd, Ad], axis=1)\n    else:\n        # keep only samples where dims == majority for BOTH modalities\n        idxs = [i for i,(v,a) in enumerate(zip(V,A)) if v.shape[0]==v_major and a.shape[0]==a_major]\n        if len(idxs)==0:\n            raise RuntimeError(\"No overlapping majority-dim samples for fusion. Consider USE_PADDING=True.\")\n        V_keep = np.stack([V[i] for i in idxs], axis=0)\n        A_keep = np.stack([A[i] for i in idxs], axis=0)\n        X = np.concatenate([V_keep, A_keep], axis=1)\n        Y = np.array([Y[i] for i in idxs], dtype=np.int64)\n        G = np.array([G[i] for i in idxs])\n    return X, Y, G\n\n# --- Build matrices ---\nXv,  Yv,  Gv  = build_V_only(usable)\nXa,  Ya,  Ga  = build_A_only(usable)\nXva, Yva, Gva = build_VA_fusion(usable)\n\nprint(\"Vision-only:\", Xv.shape, Yv.shape)\nprint(\"Audio-only :\", Xa.shape, Ya.shape)\nprint(\"Fusion     :\", Xva.shape, Yva.shape)\n\n# Little sanity check: non-empty?\nif Xv.size == 0 and Xa.size == 0:\n    print(\"WARNING: No usable features found. Check feature folders & sample_id naming.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T16:01:24.452435Z","iopub.execute_input":"2025-09-20T16:01:24.452976Z","iopub.status.idle":"2025-09-20T16:01:56.159945Z","shell.execute_reply.started":"2025-09-20T16:01:24.452938Z","shell.execute_reply":"2025-09-20T16:01:56.159307Z"}},"outputs":[{"name":"stdout","text":"Vision dim counts:\n v_dim\n2048    1081\n\nAudio  dim counts:\n a_dim\n64    1068\n\nChosen majority dims -> vision: 2048, audio: 64 (set USE_PADDING=True to pad instead)\nClasses: {'hateful': 0, 'normal': 1}\nVision-only: (1081, 2048) (1081,)\nAudio-only : (1068, 64) (1068,)\nFusion     : (1066, 2112) (1066,)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nimport numpy as np\nimport pandas as pd\n\ndef run_groupkfold(X, Y, groups, name=\"\"):\n    gkf = GroupKFold(n_splits=min(5, len(np.unique(groups))))\n    def go(clf):\n        yt, yp = [], []\n        for tr, te in gkf.split(X, Y, groups):\n            clf.fit(X[tr], Y[tr])\n            yp.append(clf.predict(X[te])); yt.append(Y[te])\n        yt = np.concatenate(yt); yp = np.concatenate(yp)\n        acc = accuracy_score(yt, yp)\n        f1m = f1_score(yt, yp, average=\"macro\")\n        rep = classification_report(yt, yp, target_names=[k for k,_ in sorted(label2id.items(), key=lambda kv: kv[1])], zero_division=0)\n        return acc, f1m, rep\n\n    lr  = LogisticRegression(penalty=\"l2\", C=1.0, solver=\"saga\", class_weight=\"balanced\", max_iter=3000, n_jobs=-1)\n    mlp = MLPClassifier(hidden_layer_sizes=(256,), activation=\"relu\", alpha=1e-4,\n                        batch_size=128, learning_rate_init=1e-3, max_iter=80, early_stopping=True, random_state=42)\n\n    acc_lr, f1_lr, rep_lr = go(lr)\n    acc_mlp, f1_mlp, rep_mlp = go(mlp)\n\n    print(f\"[{name}] LR  acc={acc_lr:.3f}  f1m={f1_lr:.3f}\")\n    print(f\"[{name}] MLP acc={acc_mlp:.3f} f1m={f1_mlp:.3f}\")\n    return {\n        \"name\": name,\n        \"LR_acc\": acc_lr, \"LR_f1m\": f1_lr, \"report_LR\": rep_lr,\n        \"MLP_acc\": acc_mlp, \"MLP_f1m\": f1_mlp, \"report_MLP\": rep_mlp\n    }\n\nresults = []\nresults.append(run_groupkfold(Xv,  Yv,  Gv,  \"Vision-only (generic)\"))\nresults.append(run_groupkfold(Xa,  Ya,  Ga,  \"Audio-only (generic)\"))\nresults.append(run_groupkfold(Xva, Yva, Gva, \"Early Fusion (V+A generic)\"))\n\nsummary = pd.DataFrame([{k:v for k,v in r.items() if isinstance(v,(int,float,str))} for r in results]).sort_values(\"MLP_f1m\", ascending=False)\nsummary_path = OUT_PROC_DIR / \"baseline_summary_generic.csv\"\nsummary.to_csv(summary_path, index=False)\nprint(\"Saved:\", summary_path)\nsummary\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T16:01:56.160695Z","iopub.execute_input":"2025-09-20T16:01:56.160882Z","iopub.status.idle":"2025-09-20T16:08:31.213935Z","shell.execute_reply.started":"2025-09-20T16:01:56.160865Z","shell.execute_reply":"2025-09-20T16:08:31.213343Z"}},"outputs":[{"name":"stdout","text":"[Vision-only (generic)] LR  acc=0.716  f1m=0.705\n[Vision-only (generic)] MLP acc=0.717 f1m=0.702\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[Audio-only (generic)] LR  acc=0.632  f1m=0.626\n[Audio-only (generic)] MLP acc=0.576 f1m=0.467\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[Early Fusion (V+A generic)] LR  acc=0.735  f1m=0.726\n[Early Fusion (V+A generic)] MLP acc=0.707 f1m=0.681\nSaved: /kaggle/working/baseline_summary_generic.csv\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                         name    LR_acc    LR_f1m  \\\n0       Vision-only (generic)  0.716004  0.705085   \n2  Early Fusion (V+A generic)  0.734522  0.726460   \n1        Audio-only (generic)  0.632022  0.625866   \n\n                                           report_LR   MLP_acc   MLP_f1m  \\\n0                precision    recall  f1-score   ...  0.716929  0.702123   \n2                precision    recall  f1-score   ...  0.707317  0.681407   \n1                precision    recall  f1-score   ...  0.575843  0.466786   \n\n                                          report_MLP  \n0                precision    recall  f1-score   ...  \n2                precision    recall  f1-score   ...  \n1                precision    recall  f1-score   ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>LR_acc</th>\n      <th>LR_f1m</th>\n      <th>report_LR</th>\n      <th>MLP_acc</th>\n      <th>MLP_f1m</th>\n      <th>report_MLP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Vision-only (generic)</td>\n      <td>0.716004</td>\n      <td>0.705085</td>\n      <td>precision    recall  f1-score   ...</td>\n      <td>0.716929</td>\n      <td>0.702123</td>\n      <td>precision    recall  f1-score   ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Early Fusion (V+A generic)</td>\n      <td>0.734522</td>\n      <td>0.726460</td>\n      <td>precision    recall  f1-score   ...</td>\n      <td>0.707317</td>\n      <td>0.681407</td>\n      <td>precision    recall  f1-score   ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Audio-only (generic)</td>\n      <td>0.632022</td>\n      <td>0.625866</td>\n      <td>precision    recall  f1-score   ...</td>\n      <td>0.575843</td>\n      <td>0.466786</td>\n      <td>precision    recall  f1-score   ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Nothing heavy to install; sklearn is preinstalled on Kaggle.\nimport sklearn, sys, numpy, pandas\nprint(\"sklearn:\", sklearn.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T16:08:31.214820Z","iopub.execute_input":"2025-09-20T16:08:31.215113Z","iopub.status.idle":"2025-09-20T16:08:31.219276Z","shell.execute_reply.started":"2025-09-20T16:08:31.215085Z","shell.execute_reply":"2025-09-20T16:08:31.218676Z"}},"outputs":[{"name":"stdout","text":"sklearn: 1.2.2\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from pathlib import Path\nimport pandas as pd\nimport numpy as np\n\n# === Paths ===\nPROC_DIR    = Path(\"/kaggle/working\")                # where you saved manifests\nMANIFEST    = PROC_DIR / \"hatemm_train_manifest.rewritten.csv\" # from the rewrite step earlier\nassert MANIFEST.exists(), f\"Missing manifest: {MANIFEST}\"\n\n# This is the INPUT folder you uploaded that contains:\n#   /vision/*.npy  and  /audio/*.npy\nCACHE_DIR = Path(\"/kaggle/input/thesis-data-processed/features\")        # <-- CHANGE to your uploaded dataset folder name\nV_DIR     = CACHE_DIR / \"vision\"\nA_DIR     = CACHE_DIR / \"audio\"\nprint(\"Manifest:\", MANIFEST)\nprint(\"Vision dir exists:\", V_DIR.exists(), V_DIR)\nprint(\"Audio  dir exists:\", A_DIR.exists(), A_DIR)\n\ndf = pd.read_csv(MANIFEST)\nprint(\"Loaded rows:\", len(df))\ndf.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T16:08:31.220092Z","iopub.execute_input":"2025-09-20T16:08:31.220699Z","iopub.status.idle":"2025-09-20T16:08:31.275390Z","shell.execute_reply.started":"2025-09-20T16:08:31.220672Z","shell.execute_reply":"2025-09-20T16:08:31.274632Z"}},"outputs":[{"name":"stdout","text":"Manifest: /kaggle/working/hatemm_train_manifest.rewritten.csv\nVision dir exists: True /kaggle/input/thesis-data-processed/features/vision\nAudio  dir exists: True /kaggle/input/thesis-data-processed/features/audio\nLoaded rows: 1083\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"  dataset              video_id  \\\n0  HateMM      hate_video_1.mp4   \n1  HateMM      hate_video_2.mp4   \n2  HateMM  non_hate_video_1.mp4   \n\n                                          video_path  start_sec  end_sec  \\\n0  /kaggle/input/thesis-dataset/Thesis_dataset/ha...        NaN      NaN   \n1  /kaggle/input/thesis-dataset/Thesis_dataset/ha...        NaN      NaN   \n2  /kaggle/input/thesis-dataset/Thesis_dataset/no...        NaN      NaN   \n\n  target_group    label  binary_offensive  \\\n0   ['Blacks']  hateful                 1   \n1   ['Blacks']  hateful                 1   \n2   ['Others']   normal                 0   \n\n                                     sample_id  video_seconds  \\\n0     HateMM__hate_video_1.mp4__0_end__hateful         94.998   \n1     HateMM__hate_video_2.mp4__0_end__hateful        129.160   \n2  HateMM__non_hate_video_1.mp4__0_end__normal        108.832   \n\n   segment_seconds  is_broken  \\\n0              NaN      False   \n1              NaN      False   \n2              NaN      False   \n\n                                    vision_feat_path  \\\n0  /kaggle/input/thesis-data-processed/features/v...   \n1  /kaggle/input/thesis-data-processed/features/v...   \n2  /kaggle/input/thesis-data-processed/features/v...   \n\n                                     audio_feat_path  has_vision  has_audio  \n0  /kaggle/input/thesis-data-processed/features/a...        True       True  \n1  /kaggle/input/thesis-data-processed/features/a...        True       True  \n2  /kaggle/input/thesis-data-processed/features/a...        True       True  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>video_id</th>\n      <th>video_path</th>\n      <th>start_sec</th>\n      <th>end_sec</th>\n      <th>target_group</th>\n      <th>label</th>\n      <th>binary_offensive</th>\n      <th>sample_id</th>\n      <th>video_seconds</th>\n      <th>segment_seconds</th>\n      <th>is_broken</th>\n      <th>vision_feat_path</th>\n      <th>audio_feat_path</th>\n      <th>has_vision</th>\n      <th>has_audio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HateMM</td>\n      <td>hate_video_1.mp4</td>\n      <td>/kaggle/input/thesis-dataset/Thesis_dataset/ha...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>['Blacks']</td>\n      <td>hateful</td>\n      <td>1</td>\n      <td>HateMM__hate_video_1.mp4__0_end__hateful</td>\n      <td>94.998</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>/kaggle/input/thesis-data-processed/features/v...</td>\n      <td>/kaggle/input/thesis-data-processed/features/a...</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HateMM</td>\n      <td>hate_video_2.mp4</td>\n      <td>/kaggle/input/thesis-dataset/Thesis_dataset/ha...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>['Blacks']</td>\n      <td>hateful</td>\n      <td>1</td>\n      <td>HateMM__hate_video_2.mp4__0_end__hateful</td>\n      <td>129.160</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>/kaggle/input/thesis-data-processed/features/v...</td>\n      <td>/kaggle/input/thesis-data-processed/features/a...</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HateMM</td>\n      <td>non_hate_video_1.mp4</td>\n      <td>/kaggle/input/thesis-dataset/Thesis_dataset/no...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>['Others']</td>\n      <td>normal</td>\n      <td>0</td>\n      <td>HateMM__non_hate_video_1.mp4__0_end__normal</td>\n      <td>108.832</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>/kaggle/input/thesis-data-processed/features/v...</td>\n      <td>/kaggle/input/thesis-data-processed/features/a...</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\n\ndef fix_col_path(series, target_root):\n    out = []\n    for p in series.fillna(\"\"):\n        base = Path(p).name if isinstance(p, str) and p else None\n        out.append(str(target_root / base) if base else np.nan)\n    return out\n\n# Ensure columns exist and rewrite to current CACHE_DIR\nif \"vision_feat_path\" not in df.columns: df[\"vision_feat_path\"] = np.nan\nif \"audio_feat_path\"  not in df.columns: df[\"audio_feat_path\"]  = np.nan\ndf[\"vision_feat_path\"] = fix_col_path(df[\"vision_feat_path\"], V_DIR)\ndf[\"audio_feat_path\"]  = fix_col_path(df[\"audio_feat_path\"],  A_DIR)\n\n# Keep only rows that have at least one modality (we’ll build 3 matrices separately)\nusable = df[(df[\"vision_feat_path\"].notna()) | (df[\"audio_feat_path\"].notna())].copy()\nprint(\"Usable rows:\", len(usable), \"/\", len(df))\n\n# Label map\nlabels_sorted = sorted(usable[\"label\"].unique())\nlabel2id = {lbl:i for i,lbl in enumerate(labels_sorted)}\nid2label = {i:lbl for lbl,i in label2id.items()}\nprint(\"Classes:\", label2id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T16:08:31.277619Z","iopub.execute_input":"2025-09-20T16:08:31.278210Z","iopub.status.idle":"2025-09-20T16:08:31.302708Z","shell.execute_reply.started":"2025-09-20T16:08:31.278190Z","shell.execute_reply":"2025-09-20T16:08:31.301940Z"}},"outputs":[{"name":"stdout","text":"Usable rows: 1083 / 1083\nClasses: {'hateful': 0, 'normal': 1}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# === TV3 (fixed) — Robust builders for V, A, and V+A ===\nimport numpy as np\nfrom pathlib import Path\n\ndef load_vec(p):\n    try:\n        p = Path(p)\n        if p.exists():\n            a = np.load(p, mmap_mode=None)\n            a = np.asarray(a).squeeze()\n            if a.ndim == 0: a = np.array([a], dtype=np.float32)\n            return a.astype(np.float32)\n    except Exception:\n        return None\n    return None\n\ndef dim_of(p):\n    v = load_vec(p)\n    return None if v is None else int(v.shape[0])\n\n# Diagnose dimensions\nusable = df[(df[\"vision_feat_path\"].notna()) | (df[\"audio_feat_path\"].notna())].copy()\nusable[\"v_dim\"] = usable[\"vision_feat_path\"].apply(dim_of)\nusable[\"a_dim\"] = usable[\"audio_feat_path\"].apply(dim_of)\n\nv_counts = usable[\"v_dim\"].dropna().astype(int).value_counts().sort_index()\na_counts = usable[\"a_dim\"].dropna().astype(int).value_counts().sort_index()\nprint(\"Vision dims:\", dict(v_counts))\nprint(\"Audio  dims:\", dict(a_counts))\n\n# Strategy\nUSE_PADDING = False  # False = keep majority dim; True = pad to max dim\nv_major = int(v_counts.idxmax()) if len(v_counts) else None\na_major = int(a_counts.idxmax()) if len(a_counts) else None\nprint(\"Majority dims -> vision:\", v_major, \"| audio:\", a_major)\n\ndef pad_stack(vectors, target=None):\n    vecs = [v for v in vectors if isinstance(v, np.ndarray)]\n    if len(vecs)==0: return np.zeros((0,0), np.float32)\n    if target is None:\n        target = max(v.shape[0] for v in vecs)\n    M = np.zeros((len(vecs), target), np.float32)\n    for i, v in enumerate(vecs):\n        d = min(target, v.shape[0])\n        M[i,:d] = v[:d]\n    return M\n\n# --- Builders separated per setup to avoid None leakage ---\n\ndef build_V_only(df_in):\n    V, Y, G = [], [], []\n    for _, r in df_in.iterrows():\n        v = load_vec(r[\"vision_feat_path\"]) if isinstance(r[\"vision_feat_path\"], str) else None\n        if v is None: \n            continue\n        if not USE_PADDING and v_major is not None and v.shape[0] != v_major:\n            continue\n        V.append(v)\n        Y.append(label2id[r[\"label\"]])\n        G.append(str(r[\"video_id\"]))\n    if not V:\n        return np.zeros((0,0), np.float32), np.zeros((0,), np.int64), np.array([])\n    X = pad_stack(V, None if USE_PADDING else v_major)\n    return X, np.array(Y, np.int64), np.array(G)\n\ndef build_A_only(df_in):\n    A, Y, G = [], [], []\n    for _, r in df_in.iterrows():\n        a = load_vec(r[\"audio_feat_path\"]) if isinstance(r[\"audio_feat_path\"], str) else None\n        if a is None: \n            continue\n        if not USE_PADDING and a_major is not None and a.shape[0] != a_major:\n            continue\n        A.append(a)\n        Y.append(label2id[r[\"label\"]])\n        G.append(str(r[\"video_id\"]))\n    if not A:\n        return np.zeros((0,0), np.float32), np.zeros((0,), np.int64), np.array([])\n    X = pad_stack(A, None if USE_PADDING else a_major)\n    return X, np.array(Y, np.int64), np.array(G)\n\ndef build_VA_fusion(df_in):\n    V, A, Y, G = [], [], [], []\n    for _, r in df_in.iterrows():\n        v = load_vec(r[\"vision_feat_path\"]) if isinstance(r[\"vision_feat_path\"], str) else None\n        a = load_vec(r[\"audio_feat_path\"])  if isinstance(r[\"audio_feat_path\"],  str) else None\n        # Fusion requires BOTH\n        if v is None or a is None:\n            continue\n        if not USE_PADDING:\n            if v_major is not None and v.shape[0] != v_major:\n                continue\n            if a_major is not None and a.shape[0] != a_major:\n                continue\n        V.append(v); A.append(a)\n        Y.append(label2id[r[\"label\"]])\n        G.append(str(r[\"video_id\"]))\n    if not V:\n        return np.zeros((0,0), np.float32), np.zeros((0,), np.int64), np.array([])\n    V_mat = pad_stack(V, None if USE_PADDING else v_major)\n    A_mat = pad_stack(A, None if USE_PADDING else a_major)\n    X = np.concatenate([V_mat, A_mat], axis=1)\n    return X, np.array(Y, np.int64), np.array(G)\n\n# Label map (ensure exists)\nlabels_sorted = sorted(usable[\"label\"].unique())\nlabel2id = {lbl:i for i,lbl in enumerate(labels_sorted)}\nid2label = {i:lbl for lbl,i in label2id.items()}\nprint(\"Classes:\", label2id)\n\n# Build three sets\nXv,  Yv,  Gv  = build_V_only(usable)\nXa,  Ya,  Ga  = build_A_only(usable)\nXva, Yva, Gva = build_VA_fusion(usable)\n\nprint(\"Vision-only:\", Xv.shape,  \"labels:\", len(Yv))\nprint(\"Audio-only :\", Xa.shape,  \"labels:\", len(Ya))\nprint(\"V+A fusion :\", Xva.shape, \"labels:\", len(Yva))\nif Xv.size==0 and Xa.size==0 and Xva.size==0:\n    print(\"WARNING: No usable features found. Recheck file names and paths.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T16:08:31.303544Z","iopub.execute_input":"2025-09-20T16:08:31.303823Z","iopub.status.idle":"2025-09-20T16:08:44.988872Z","shell.execute_reply.started":"2025-09-20T16:08:31.303805Z","shell.execute_reply":"2025-09-20T16:08:44.988112Z"}},"outputs":[{"name":"stdout","text":"Vision dims: {2048: 1081}\nAudio  dims: {64: 1068}\nMajority dims -> vision: 2048 | audio: 64\nClasses: {'hateful': 0, 'normal': 1}\nVision-only: (1081, 2048) labels: 1081\nAudio-only : (1068, 64) labels: 1068\nV+A fusion : (1066, 2112) labels: 1066\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nimport numpy as np\nimport pandas as pd\n\ndef eval_groupkfold(X, Y, G, name):\n    if X.size == 0:\n        print(f\"{name}: EMPTY, skip.\")\n        return {\"name\":name, \"LR_acc\":np.nan, \"LR_f1m\":np.nan, \"MLP_acc\":np.nan, \"MLP_f1m\":np.nan}\n\n    gkf = GroupKFold(n_splits=min(5, len(np.unique(G))))\n    def run(clf):\n        yt, yp = [], []\n        for tr, te in gkf.split(X, Y, G):\n            clf.fit(X[tr], Y[tr])\n            yp.append(clf.predict(X[te])); yt.append(Y[te])\n        yt = np.concatenate(yt); yp = np.concatenate(yp)\n        acc = accuracy_score(yt, yp)\n        f1m = f1_score(yt, yp, average=\"macro\")\n        rep = classification_report(yt, yp, target_names=[k for k,_ in sorted(label2id.items(), key=lambda kv: kv[1])], zero_division=0)\n        return acc, f1m, rep\n\n    lr  = LogisticRegression(penalty=\"l2\", C=1.0, solver=\"saga\", class_weight=\"balanced\", max_iter=3000, n_jobs=-1)\n    mlp = MLPClassifier(hidden_layer_sizes=(256,), activation=\"relu\", alpha=1e-4,\n                        batch_size=128, learning_rate_init=1e-3, max_iter=80, early_stopping=True, random_state=42)\n\n    acc_lr, f1_lr, rep_lr     = run(lr)\n    acc_mlp, f1_mlp, rep_mlp  = run(mlp)\n\n    print(f\"[{name}]  LR : acc={acc_lr:.3f}  f1m={f1_lr:.3f}\")\n    print(rep_lr)\n    print(f\"[{name}]  MLP: acc={acc_mlp:.3f} f1m={f1_mlp:.3f}\")\n    print(rep_mlp)\n\n    return {\"name\":name, \"LR_acc\":acc_lr, \"LR_f1m\":f1_lr, \"report_LR\":rep_lr,\n            \"MLP_acc\":acc_mlp, \"MLP_f1m\":f1_mlp, \"report_MLP\":rep_mlp}\n\nrows = []\nrows.append(eval_groupkfold(Xv,  Yv,  Gv,  \"Vision-only\"))\nrows.append(eval_groupkfold(Xa,  Ya,  Ga,  \"Audio-only\"))\nrows.append(eval_groupkfold(Xva, Yva, Gva, \"V+A (Early Fusion)\"))\n\nsummary = pd.DataFrame([{k:v for k,v in r.items() if isinstance(v,(int,float,str))} for r in rows]).sort_values(\"MLP_f1m\", ascending=False)\nOUT = PROC_DIR / \"baseline_summary_VA.csv\"\nsummary.to_csv(OUT, index=False)\nprint(\"Saved:\", OUT)\nsummary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T16:08:44.989679Z","iopub.execute_input":"2025-09-20T16:08:44.989864Z","iopub.status.idle":"2025-09-20T16:15:17.672613Z","shell.execute_reply.started":"2025-09-20T16:08:44.989848Z","shell.execute_reply":"2025-09-20T16:15:17.671795Z"}},"outputs":[{"name":"stdout","text":"[Vision-only]  LR : acc=0.716  f1m=0.705\n              precision    recall  f1-score   support\n\n     hateful       0.64      0.66      0.65       429\n      normal       0.77      0.75      0.76       652\n\n    accuracy                           0.72      1081\n   macro avg       0.70      0.71      0.71      1081\nweighted avg       0.72      0.72      0.72      1081\n\n[Vision-only]  MLP: acc=0.717 f1m=0.702\n              precision    recall  f1-score   support\n\n     hateful       0.65      0.62      0.64       429\n      normal       0.76      0.78      0.77       652\n\n    accuracy                           0.72      1081\n   macro avg       0.70      0.70      0.70      1081\nweighted avg       0.72      0.72      0.72      1081\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[Audio-only]  LR : acc=0.632  f1m=0.626\n              precision    recall  f1-score   support\n\n     hateful       0.54      0.63      0.58       429\n      normal       0.72      0.64      0.67       639\n\n    accuracy                           0.63      1068\n   macro avg       0.63      0.63      0.63      1068\nweighted avg       0.64      0.63      0.64      1068\n\n[Audio-only]  MLP: acc=0.576 f1m=0.467\n              precision    recall  f1-score   support\n\n     hateful       0.42      0.15      0.23       429\n      normal       0.60      0.86      0.71       639\n\n    accuracy                           0.58      1068\n   macro avg       0.51      0.51      0.47      1068\nweighted avg       0.53      0.58      0.51      1068\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[V+A (Early Fusion)]  LR : acc=0.735  f1m=0.726\n              precision    recall  f1-score   support\n\n     hateful       0.66      0.70      0.68       427\n      normal       0.79      0.76      0.77       639\n\n    accuracy                           0.73      1066\n   macro avg       0.72      0.73      0.73      1066\nweighted avg       0.74      0.73      0.74      1066\n\n[V+A (Early Fusion)]  MLP: acc=0.707 f1m=0.681\n              precision    recall  f1-score   support\n\n     hateful       0.67      0.53      0.59       427\n      normal       0.72      0.83      0.77       639\n\n    accuracy                           0.71      1066\n   macro avg       0.70      0.68      0.68      1066\nweighted avg       0.70      0.71      0.70      1066\n\nSaved: /kaggle/working/baseline_summary_VA.csv\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                 name    LR_acc    LR_f1m  \\\n0         Vision-only  0.716004  0.705085   \n2  V+A (Early Fusion)  0.734522  0.726460   \n1          Audio-only  0.632022  0.625866   \n\n                                           report_LR   MLP_acc   MLP_f1m  \\\n0                precision    recall  f1-score   ...  0.716929  0.702123   \n2                precision    recall  f1-score   ...  0.707317  0.681407   \n1                precision    recall  f1-score   ...  0.575843  0.466786   \n\n                                          report_MLP  \n0                precision    recall  f1-score   ...  \n2                precision    recall  f1-score   ...  \n1                precision    recall  f1-score   ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>LR_acc</th>\n      <th>LR_f1m</th>\n      <th>report_LR</th>\n      <th>MLP_acc</th>\n      <th>MLP_f1m</th>\n      <th>report_MLP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Vision-only</td>\n      <td>0.716004</td>\n      <td>0.705085</td>\n      <td>precision    recall  f1-score   ...</td>\n      <td>0.716929</td>\n      <td>0.702123</td>\n      <td>precision    recall  f1-score   ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>V+A (Early Fusion)</td>\n      <td>0.734522</td>\n      <td>0.726460</td>\n      <td>precision    recall  f1-score   ...</td>\n      <td>0.707317</td>\n      <td>0.681407</td>\n      <td>precision    recall  f1-score   ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Audio-only</td>\n      <td>0.632022</td>\n      <td>0.625866</td>\n      <td>precision    recall  f1-score   ...</td>\n      <td>0.575843</td>\n      <td>0.466786</td>\n      <td>precision    recall  f1-score   ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from itertools import islice\n\nprint(\"Sample feature files in vision/:\")\nfor p in islice(sorted(V_DIR.glob(\"*.npy\")), 5):\n    print(\" -\", p.name)\n\nprint(\"\\nSample feature files in audio/:\")\nfor p in islice(sorted(A_DIR.glob(\"*.npy\")), 5):\n    print(\" -\", p.name)\n\nprint(\"\\nSample rows from manifest (paths normalized):\")\ndf[[\"sample_id\",\"video_id\",\"label\",\"vision_feat_path\",\"audio_feat_path\"]].head(5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T16:15:17.673513Z","iopub.execute_input":"2025-09-20T16:15:17.674174Z","iopub.status.idle":"2025-09-20T16:15:18.159132Z","shell.execute_reply.started":"2025-09-20T16:15:17.674155Z","shell.execute_reply":"2025-09-20T16:15:18.158291Z"}},"outputs":[{"name":"stdout","text":"Sample feature files in vision/:\n - HateMM__hate_video_1.mp4__0_end__hateful.npy\n - HateMM__hate_video_10.mp4__0_end__hateful.npy\n - HateMM__hate_video_100.mp4__0_end__hateful.npy\n - HateMM__hate_video_101.mp4__0_end__hateful.npy\n - HateMM__hate_video_102.mp4__0_end__hateful.npy\n\nSample feature files in audio/:\n - HateMM__hate_video_1.mp4__0_end__hateful.npy\n - HateMM__hate_video_10.mp4__0_end__hateful.npy\n - HateMM__hate_video_100.mp4__0_end__hateful.npy\n - HateMM__hate_video_101.mp4__0_end__hateful.npy\n - HateMM__hate_video_102.mp4__0_end__hateful.npy\n\nSample rows from manifest (paths normalized):\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                     sample_id              video_id    label  \\\n0     HateMM__hate_video_1.mp4__0_end__hateful      hate_video_1.mp4  hateful   \n1     HateMM__hate_video_2.mp4__0_end__hateful      hate_video_2.mp4  hateful   \n2  HateMM__non_hate_video_1.mp4__0_end__normal  non_hate_video_1.mp4   normal   \n3     HateMM__hate_video_3.mp4__0_end__hateful      hate_video_3.mp4  hateful   \n4  HateMM__non_hate_video_2.mp4__0_end__normal  non_hate_video_2.mp4   normal   \n\n                                    vision_feat_path  \\\n0  /kaggle/input/thesis-data-processed/features/v...   \n1  /kaggle/input/thesis-data-processed/features/v...   \n2  /kaggle/input/thesis-data-processed/features/v...   \n3  /kaggle/input/thesis-data-processed/features/v...   \n4  /kaggle/input/thesis-data-processed/features/v...   \n\n                                     audio_feat_path  \n0  /kaggle/input/thesis-data-processed/features/a...  \n1  /kaggle/input/thesis-data-processed/features/a...  \n2  /kaggle/input/thesis-data-processed/features/a...  \n3  /kaggle/input/thesis-data-processed/features/a...  \n4  /kaggle/input/thesis-data-processed/features/a...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>video_id</th>\n      <th>label</th>\n      <th>vision_feat_path</th>\n      <th>audio_feat_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HateMM__hate_video_1.mp4__0_end__hateful</td>\n      <td>hate_video_1.mp4</td>\n      <td>hateful</td>\n      <td>/kaggle/input/thesis-data-processed/features/v...</td>\n      <td>/kaggle/input/thesis-data-processed/features/a...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HateMM__hate_video_2.mp4__0_end__hateful</td>\n      <td>hate_video_2.mp4</td>\n      <td>hateful</td>\n      <td>/kaggle/input/thesis-data-processed/features/v...</td>\n      <td>/kaggle/input/thesis-data-processed/features/a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HateMM__non_hate_video_1.mp4__0_end__normal</td>\n      <td>non_hate_video_1.mp4</td>\n      <td>normal</td>\n      <td>/kaggle/input/thesis-data-processed/features/v...</td>\n      <td>/kaggle/input/thesis-data-processed/features/a...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HateMM__hate_video_3.mp4__0_end__hateful</td>\n      <td>hate_video_3.mp4</td>\n      <td>hateful</td>\n      <td>/kaggle/input/thesis-data-processed/features/v...</td>\n      <td>/kaggle/input/thesis-data-processed/features/a...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HateMM__non_hate_video_2.mp4__0_end__normal</td>\n      <td>non_hate_video_2.mp4</td>\n      <td>normal</td>\n      <td>/kaggle/input/thesis-data-processed/features/v...</td>\n      <td>/kaggle/input/thesis-data-processed/features/a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from pathlib import Path\nimport pandas as pd\nimport numpy as np\n\n# === đường dẫn dữ liệu đã chuẩn hoá (từ notebook preprocess) ===\nPROC_DIR    = Path(\"/kaggle/working\")\nMANIFEST_IN = PROC_DIR / \"hatemm_train_manifest.rewritten.csv\"  # đã rewrite sang CACHE_DIR/input\n\nassert MANIFEST_IN.exists(), \"Thiếu manifest. Hãy chạy notebook preprocessing + rewrite trước.\"\n\n# === nơi đọc feature đã cache (đã upload sang /kaggle/input/<dataset>) ===\n# VD: /kaggle/input/thesis-data-processed chứa 2 thư mục vision/ và audio/\nCACHE_DIR = Path(\"/kaggle/input/thesis-data-processed/features\")   # <-- đổi theo tên dataset input của bạn\nV_DIR = CACHE_DIR / \"vision\"\nA_DIR = CACHE_DIR / \"audio\"\nT_DIR = CACHE_DIR / \"text\"    # sẽ sinh mới (embedding .npy) nhưng vì /kaggle/input read-only -> lưu sang working\n\n# === nơi ghi output mới (có quyền ghi) ===\nWORK_CACHE = Path(\"/kaggle/working/cache_hatemm_plus\")\nV_DIR_WORK = WORK_CACHE / \"vision\" ; V_DIR_WORK.mkdir(parents=True, exist_ok=True)\nA_DIR_WORK = WORK_CACHE / \"audio\"  ; A_DIR_WORK.mkdir(parents=True, exist_ok=True)\nT_DIR_WORK = WORK_CACHE / \"text\"   ; T_DIR_WORK.mkdir(parents=True, exist_ok=True)\n\n# manifest\ndf = pd.read_csv(MANIFEST_IN)\nprint(\"Loaded rows:\", len(df))\ndf.head(2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T16:15:18.160044Z","iopub.execute_input":"2025-09-20T16:15:18.160309Z","iopub.status.idle":"2025-09-20T16:15:18.188298Z","shell.execute_reply.started":"2025-09-20T16:15:18.160286Z","shell.execute_reply":"2025-09-20T16:15:18.187716Z"}},"outputs":[{"name":"stdout","text":"Loaded rows: 1083\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"  dataset          video_id  \\\n0  HateMM  hate_video_1.mp4   \n1  HateMM  hate_video_2.mp4   \n\n                                          video_path  start_sec  end_sec  \\\n0  /kaggle/input/thesis-dataset/Thesis_dataset/ha...        NaN      NaN   \n1  /kaggle/input/thesis-dataset/Thesis_dataset/ha...        NaN      NaN   \n\n  target_group    label  binary_offensive  \\\n0   ['Blacks']  hateful                 1   \n1   ['Blacks']  hateful                 1   \n\n                                  sample_id  video_seconds  segment_seconds  \\\n0  HateMM__hate_video_1.mp4__0_end__hateful         94.998              NaN   \n1  HateMM__hate_video_2.mp4__0_end__hateful        129.160              NaN   \n\n   is_broken                                   vision_feat_path  \\\n0      False  /kaggle/input/thesis-data-processed/features/v...   \n1      False  /kaggle/input/thesis-data-processed/features/v...   \n\n                                     audio_feat_path  has_vision  has_audio  \n0  /kaggle/input/thesis-data-processed/features/a...        True       True  \n1  /kaggle/input/thesis-data-processed/features/a...        True       True  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>video_id</th>\n      <th>video_path</th>\n      <th>start_sec</th>\n      <th>end_sec</th>\n      <th>target_group</th>\n      <th>label</th>\n      <th>binary_offensive</th>\n      <th>sample_id</th>\n      <th>video_seconds</th>\n      <th>segment_seconds</th>\n      <th>is_broken</th>\n      <th>vision_feat_path</th>\n      <th>audio_feat_path</th>\n      <th>has_vision</th>\n      <th>has_audio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HateMM</td>\n      <td>hate_video_1.mp4</td>\n      <td>/kaggle/input/thesis-dataset/Thesis_dataset/ha...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>['Blacks']</td>\n      <td>hateful</td>\n      <td>1</td>\n      <td>HateMM__hate_video_1.mp4__0_end__hateful</td>\n      <td>94.998</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>/kaggle/input/thesis-data-processed/features/v...</td>\n      <td>/kaggle/input/thesis-data-processed/features/a...</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HateMM</td>\n      <td>hate_video_2.mp4</td>\n      <td>/kaggle/input/thesis-dataset/Thesis_dataset/ha...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>['Blacks']</td>\n      <td>hateful</td>\n      <td>1</td>\n      <td>HateMM__hate_video_2.mp4__0_end__hateful</td>\n      <td>129.160</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>/kaggle/input/thesis-data-processed/features/v...</td>\n      <td>/kaggle/input/thesis-data-processed/features/a...</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Giữ nguyên file ở /kaggle/input, chỉ \"viết lại path\" trỏ tới đúng nơi đọc\n# Để thống nhất, chúng ta cho vision/audio path dùng trực tiếp từ INPUT; text sẽ nằm ở WORKING.\ndef fix_col_path(series, target_root):\n    out = []\n    for p in series.fillna(\"\"):\n        base = Path(p).name if isinstance(p, str) and p else None\n        if base:\n            out.append(str(target_root / base))\n        else:\n            out.append(np.nan)\n    return out\n\ndf[\"vision_feat_path\"] = fix_col_path(df.get(\"vision_feat_path\", pd.Series([np.nan]*len(df))), V_DIR)\ndf[\"audio_feat_path\"]  = fix_col_path(df.get(\"audio_feat_path\",  pd.Series([np.nan]*len(df))), A_DIR)\n\n# Đảm bảo sample_id tồn tại (dự phòng)\nif \"sample_id\" not in df.columns:\n    def make_sid(r):\n        s = \"0\" if pd.isna(r.get(\"start_sec\")) else f\"{float(r['start_sec']):.3f}\"\n        e = \"end\" if pd.isna(r.get(\"end_sec\")) else f\"{float(r['end_sec']):.3f}\"\n        return f\"{r.get('dataset','HateMM')}__{r['video_id']}__{s}_{e}__{r['label']}\"\n    df[\"sample_id\"] = df.apply(make_sid, axis=1)\n\nprint(\"vision non-null ratio:\", df[\"vision_feat_path\"].notna().mean())\nprint(\"audio  non-null ratio:\", df[\"audio_feat_path\"].notna().mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T16:15:18.189140Z","iopub.execute_input":"2025-09-20T16:15:18.189406Z","iopub.status.idle":"2025-09-20T16:15:18.214154Z","shell.execute_reply.started":"2025-09-20T16:15:18.189390Z","shell.execute_reply":"2025-09-20T16:15:18.213449Z"}},"outputs":[{"name":"stdout","text":"vision non-null ratio: 1.0\naudio  non-null ratio: 1.0\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import subprocess, shlex, pandas as pd, numpy as np\nimport whisper, torch\nfrom pathlib import Path\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nWHISPER_MODEL = \"small\"  # tiny/base/small; nếu hạn chế compute -> dùng \"tiny\" hoặc \"base\"\nasr = whisper.load_model(WHISPER_MODEL, device=device)\n\n# Ta cần file video gốc để cắt theo (start_sec, end_sec).\n# Nếu manifest có cột 'video_path' -> dùng trực tiếp.\n# Nếu không, bạn phải thêm cột đó trước (mapping từ video_id -> đường dẫn thực tế).\nassert \"video_path\" in df.columns, \"Thiếu 'video_path' trong manifest. Cần cột này để ASR.\"\n\nWAV_TMP = Path(\"/kaggle/working/asr_wav\")\nWAV_TMP.mkdir(parents=True, exist_ok=True)\n\ndef ffmpeg_extract_wav(video_path: str, out_wav: Path, start=None, end=None, sr=16000):\n    ss = f\"-ss {float(start):.3f}\" if start is not None and not pd.isna(start) else \"\"\n    to = f\"-to {float(end):.3f}\"   if end is not None and not pd.isna(end)   else \"\"\n    cmd = f'ffmpeg -hide_banner -loglevel error -y {ss} -i {shlex.quote(video_path)} {to} -vn -ac 1 -ar {sr} {shlex.quote(str(out_wav))}'\n    ok = subprocess.run(cmd, shell=True).returncode == 0\n    return str(out_wav) if ok and out_wav.exists() and out_wav.stat().st_size>0 else None\n\ndef asr_one(row):\n    sid = row[\"sample_id\"]\n    s, e = row.get(\"start_sec\"), row.get(\"end_sec\")\n    vpath = row[\"video_path\"]\n    wavp = WAV_TMP / f\"{sid}.wav\"\n    wav_file = ffmpeg_extract_wav(vpath, wavp, s, e, sr=16000)\n    if wav_file is None: return None\n    # whisper transcribe\n    try:\n        out = asr.transcribe(wav_file, language=\"en\")  # nếu video không phải tiếng Anh, bỏ language để auto\n        text = (out.get(\"text\") or \"\").strip()\n        return text\n    except Exception:\n        return None\n    finally:\n        try: wavp.unlink()\n        except: pass\n\n# ASR (có thể tốn thời gian; chạy theo batches/limit nếu cần)\nif \"transcript\" not in df.columns:\n    df[\"transcript\"] = None\n\ntodo_idx = df[df[\"transcript\"].isna()].index\nprint(\"ASR pending:\", len(todo_idx))\nfor i in todo_idx:\n    t = asr_one(df.loc[i])\n    df.at[i, \"transcript\"] = t\n    if (i % 50) == 0:\n        print(f\"ASR progress i={i}, got={t[:60] if t else None}\")\n\n# Lưu lại intermediate transcript\nTRANS_OUT = PROC_DIR / \"hatemm_transcripts.csv\"\ndf[[\"sample_id\",\"video_id\",\"label\",\"transcript\"]].to_csv(TRANS_OUT, index=False)\nprint(\"Saved transcripts:\", TRANS_OUT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T16:15:18.214893Z","iopub.execute_input":"2025-09-20T16:15:18.215180Z","iopub.status.idle":"2025-09-20T19:46:19.156256Z","shell.execute_reply.started":"2025-09-20T16:15:18.215158Z","shell.execute_reply":"2025-09-20T19:46:19.155538Z"}},"outputs":[{"name":"stderr","text":"100%|███████████████████████████████████████| 461M/461M [00:06<00:00, 79.9MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"ASR pending: 1083\nASR progress i=0, got=You You You You\n","output_type":"stream"},{"name":"stderr","text":"Output file #0 does not contain any stream\nOutput file #0 does not contain any stream\nOutput file #0 does not contain any stream\n","output_type":"stream"},{"name":"stdout","text":"ASR progress i=50, got=Cancer nigger, how does it feel to be dying of a Jewish dise\nASR progress i=100, got=Now, how do I know that the white people know that we are go\n","output_type":"stream"},{"name":"stderr","text":"Output file #0 does not contain any stream\n","output_type":"stream"},{"name":"stdout","text":"ASR progress i=150, got=Sitting in my... oh fuck. Hello? What about GCC? I can't und\nASR progress i=200, got=Okay, give me a favor, come over here. Go place your hands b\n","output_type":"stream"},{"name":"stderr","text":"Output file #0 does not contain any stream\n[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5ab594ba8500] stream 1, offset 0x23ce62e: partial file\n/kaggle/input/thesis-dataset/Thesis_dataset/hate_videos/hate_video_95.mp4: Invalid data found when processing input\n","output_type":"stream"},{"name":"stdout","text":"ASR progress i=250, got=The first one is the left side of the right side of the righ\n","output_type":"stream"},{"name":"stderr","text":"Output file #0 does not contain any stream\n","output_type":"stream"},{"name":"stdout","text":"ASR progress i=300, got=You fat, stinking and you ugly. You fat, you stinking and yo\nASR progress i=350, got=That's a little horseshit that you fucking can be made of. Y\n","output_type":"stream"},{"name":"stderr","text":"Output file #0 does not contain any stream\n","output_type":"stream"},{"name":"stdout","text":"ASR progress i=400, got=What? I have a chemical firearm on me. So you niggers gonna \n","output_type":"stream"},{"name":"stderr","text":"Output file #0 does not contain any stream\nOutput file #0 does not contain any stream\nOutput file #0 does not contain any stream\n","output_type":"stream"},{"name":"stdout","text":"ASR progress i=450, got=Alrighty, we are going to learn how to put a MailChimp subsc\nASR progress i=500, got=Hey yo I'm getting ready to put y'all up on something man. W\nASR progress i=550, got=I don't give a fuck, not a single fuck, not a single solitar\nASR progress i=600, got=laughs I upset I'm not sure what I'm doing. I'm not sure wha\nASR progress i=650, got=P간ar\n","output_type":"stream"},{"name":"stderr","text":"Output file #0 does not contain any stream\n","output_type":"stream"},{"name":"stdout","text":"ASR progress i=700, got=Bring that bell, shout for joy, white men's dish here, Gathe\n","output_type":"stream"},{"name":"stderr","text":"Output file #0 does not contain any stream\nOutput file #0 does not contain any stream\n","output_type":"stream"},{"name":"stdout","text":"ASR progress i=750, got=The football team coming back in 2011. So you know, he likes\nASR progress i=800, got=How can we ever be expected to be expected as a people, as a\nASR progress i=850, got=Trump supporters just took my job away from me. I have gotte\nASR progress i=900, got=Before I was even born, my father was in the Congress of the\n","output_type":"stream"},{"name":"stderr","text":"Output file #0 does not contain any stream\n","output_type":"stream"},{"name":"stdout","text":"ASR progress i=950, got=Here's Dan, riding in the fire, riding in crosses by the nig\n","output_type":"stream"},{"name":"stderr","text":"Output file #0 does not contain any stream\n","output_type":"stream"},{"name":"stdout","text":"ASR progress i=1000, got=Well boys, as far as we go... This will allow space sensors \nASR progress i=1050, got=紅藏兒隊! . . . . . . . . . , . . . . . . . . .aines de No, God \nSaved transcripts: /kaggle/working/hatemm_transcripts.csv\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import torch, numpy as np\nfrom transformers import AutoTokenizer, AutoModel\nfrom tqdm import tqdm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# BERT-base-uncased. Nếu không có internet, hãy Add Data một dataset chứa model này,\n# sau đó đổi model_name thành đường dẫn local (e.g., \"/kaggle/input/bert-base-uncased\").\nmodel_name = \"bert-base-uncased\"\ntok = AutoTokenizer.from_pretrained(model_name)\nbert = AutoModel.from_pretrained(model_name).to(device)\nbert.eval()\n\ndef mean_pool(last_hidden_state, attention_mask):\n    mask = attention_mask.unsqueeze(-1).type(last_hidden_state.dtype)\n    masked = last_hidden_state * mask\n    return masked.sum(dim=1) / mask.sum(dim=1).clamp(min=1e-6)\n\ntext_paths = []\nfor i, r in tqdm(df.iterrows(), total=len(df)):\n    sid = r[\"sample_id\"]\n    txt = str(r.get(\"transcript\") or \"\").strip()\n    if not txt:\n        text_paths.append(np.nan)\n        continue\n    out_path = T_DIR_WORK / f\"{sid}.npy\"\n    if out_path.exists():\n        text_paths.append(str(out_path)); continue\n    try:\n        enc = tok(txt, truncation=True, max_length=256, padding=True, return_tensors=\"pt\")\n        enc = {k:v.to(device) for k,v in enc.items()}\n        with torch.no_grad():\n            out = bert(**enc).last_hidden_state  # (1, L, H)\n            emb = mean_pool(out, enc[\"attention_mask\"]).squeeze(0).detach().cpu().numpy().astype(np.float32)  # (H,)\n        np.save(out_path, emb)\n        text_paths.append(str(out_path))\n    except Exception:\n        text_paths.append(np.nan)\n\ndf[\"text_feat_path\"] = text_paths\nprint(\"text non-null ratio:\", df[\"text_feat_path\"].notna().mean())\n\n# Lưu manifest mới có text\nMANIFEST_TXT = PROC_DIR / \"hatemm_train_manifest.with_text.csv\"\ndf.to_csv(MANIFEST_TXT, index=False)\nprint(\"Saved:\", MANIFEST_TXT)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T19:46:19.157043Z","iopub.execute_input":"2025-09-20T19:46:19.157268Z","iopub.status.idle":"2025-09-20T19:46:42.949820Z","shell.execute_reply.started":"2025-09-20T19:46:19.157249Z","shell.execute_reply":"2025-09-20T19:46:42.949124Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b929f21b93e0453080caaa946627cb6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f350922f53f4d109780dee44025e88d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f68dd71f04e34a5aa81b0fd47568e672"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3b3bb5738f5452b9f73dc6ba20d5dd8"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eebfe524260e430ebc608c9ccacfb8aa"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 1083/1083 [00:16<00:00, 65.13it/s]","output_type":"stream"},{"name":"stdout","text":"text non-null ratio: 0.971375807940905\nSaved: /kaggle/working/hatemm_train_manifest.with_text.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\ndef load_vec(p):\n    try:\n        p = Path(p)\n        if p.exists():\n            a = np.load(p, mmap_mode=None)\n            return a.squeeze().astype(np.float32)\n    except Exception:\n        return None\n    return None\n\ndef dim_of(p):\n    try:\n        a = load_vec(p)\n        return None if a is None else a.shape[0]\n    except: return None\n\nuse = df.copy()\nuse[\"v_dim\"] = use[\"vision_feat_path\"].apply(dim_of)\nuse[\"a_dim\"] = use[\"audio_feat_path\"].apply(dim_of)\nuse[\"t_dim\"] = use[\"text_feat_path\"].apply(dim_of)\n\nv_counts = use[\"v_dim\"].dropna().astype(int).value_counts()\na_counts = use[\"a_dim\"].dropna().astype(int).value_counts()\nt_counts = use[\"t_dim\"].dropna().astype(int).value_counts()\nprint(\"V dim:\", v_counts.to_dict())\nprint(\"A dim:\", a_counts.to_dict())\nprint(\"T dim:\", t_counts.to_dict())\n\nUSE_PADDING = False  # True: pad mỗi modality đến max; False: chỉ giữ majority-dim cho từng modality\n\nv_major = int(v_counts.idxmax()) if len(v_counts) else None\na_major = int(a_counts.idxmax()) if len(a_counts) else None\nt_major = int(t_counts.idxmax()) if len(t_counts) else None\nprint(\"major dims ->\", v_major, a_major, t_major)\n\nlabels_sorted = sorted(use[\"label\"].unique())\nlabel2id = {lbl:i for i,lbl in enumerate(labels_sorted)}\n\ndef pad_stack(vectors, target=None):\n    if len(vectors)==0: return np.zeros((0,0), np.float32)\n    if target is None: target = max(v.shape[0] for v in vectors)\n    M = np.zeros((len(vectors), target), np.float32)\n    for i, v in enumerate(vectors):\n        d = min(target, v.shape[0])\n        M[i,:d] = v[:d]\n    return M\n\ndef collect(mods=(\"v\",\"a\",\"t\")):\n    rows = []\n    for _, r in use.iterrows():\n        feats = []\n        dims_ok = True\n        if \"v\" in mods:\n            v = load_vec(r[\"vision_feat_path\"])\n            if v is None: dims_ok=False\n            elif not USE_PADDING and v.shape[0]!=v_major: dims_ok=False\n            else: feats.append((\"v\", v))\n        if \"a\" in mods:\n            a = load_vec(r[\"audio_feat_path\"])\n            if a is None: dims_ok=False\n            elif not USE_PADDING and a.shape[0]!=a_major: dims_ok=False\n            else: feats.append((\"a\", a))\n        if \"t\" in mods:\n            t = load_vec(r[\"text_feat_path\"])\n            if t is None: dims_ok=False\n            elif not USE_PADDING and t.shape[0]!=t_major: dims_ok=False\n            else: feats.append((\"t\", t))\n        if not dims_ok or not feats: continue\n        rows.append((feats, label2id[r[\"label\"]], str(r[\"video_id\"])))\n    if not rows:\n        return np.zeros((0,0), np.float32), np.zeros((0,), np.int64), np.array([])\n    Xs = {\"v\":[], \"a\":[], \"t\":[]}\n    Y, G = [], []\n    for feats, y, g in rows:\n        for k, vec in feats:\n            Xs[k].append(vec)\n        Y.append(y); G.append(g)\n    # pad per-modality then concat in fixed order\n    mats = []\n    for k in mods:\n        if len(Xs[k])==0: continue\n        if USE_PADDING:\n            mats.append(pad_stack(Xs[k]))\n        else:\n            target = {\"v\":v_major, \"a\":a_major, \"t\":t_major}[k]\n            mats.append(pad_stack(Xs[k], target))\n    X = np.concatenate(mats, axis=1) if len(mats)>1 else mats[0]\n    Y = np.array(Y, np.int64); G = np.array(G)\n    return X, Y, G\n\n# Build all combos\nXv,  Yv,  Gv  = collect((\"v\",))\nXa,  Ya,  Ga  = collect((\"a\",))\nXt,  Yt,  Gt  = collect((\"t\",))\nXva, Yva, Gva = collect((\"v\",\"a\"))\nXvt, Yvt, Gvt = collect((\"v\",\"t\"))\nXat, Yat, Gat = collect((\"a\",\"t\"))\nXvat, Yvat, Gvat = collect((\"v\",\"a\",\"t\"))\n\nprint(\"Shapes:\")\nfor name, X, Y in [\n    (\"V\", Xv, Yv), (\"A\", Xa, Ya), (\"T\", Xt, Yt),\n    (\"V+A\", Xva, Yva), (\"V+T\", Xvt, Yvt), (\"A+T\", Xat, Yat),\n    (\"V+A+T\", Xvat, Yvat)\n]:\n    print(f\"{name:6s}: {X.shape}  labels={len(Y)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T19:46:42.950632Z","iopub.execute_input":"2025-09-20T19:46:42.950894Z","iopub.status.idle":"2025-09-20T19:47:04.106331Z","shell.execute_reply.started":"2025-09-20T19:46:42.950870Z","shell.execute_reply":"2025-09-20T19:47:04.105528Z"}},"outputs":[{"name":"stdout","text":"V dim: {2048: 1081}\nA dim: {64: 1068}\nT dim: {768: 1052}\nmajor dims -> 2048 64 768\nShapes:\nV     : (1081, 2048)  labels=1081\nA     : (1068, 64)  labels=1068\nT     : (1052, 768)  labels=1052\nV+A   : (1066, 2112)  labels=1066\nV+T   : (1050, 2816)  labels=1050\nA+T   : (1052, 832)  labels=1052\nV+A+T : (1050, 2880)  labels=1050\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nimport numpy as np, pandas as pd\n\ndef eval_block(X, Y, G, name):\n    if X.size==0:\n        return {\"Model\":name, \"LR_acc\":np.nan, \"LR_f1\":np.nan, \"MLP_acc\":np.nan, \"MLP_f1\":np.nan}\n    gkf = GroupKFold(n_splits=min(5, len(np.unique(G))))\n    def go(clf):\n        yt, yp = [], []\n        for tr, te in gkf.split(X, Y, G):\n            clf.fit(X[tr], Y[tr])\n            yp.append(clf.predict(X[te])); yt.append(Y[te])\n        yt = np.concatenate(yt); yp = np.concatenate(yp)\n        return accuracy_score(yt, yp), f1_score(yt, yp, average=\"macro\")\n    lr  = LogisticRegression(penalty=\"l2\", C=1.0, solver=\"saga\", class_weight=\"balanced\", max_iter=3000, n_jobs=-1)\n    mlp = MLPClassifier(hidden_layer_sizes=(256,), activation=\"relu\", alpha=1e-4,\n                        batch_size=128, learning_rate_init=1e-3, max_iter=80, early_stopping=True, random_state=42)\n    acc_lr, f1_lr = go(lr)\n    acc_mlp, f1_mlp = go(mlp)\n    print(f\"{name:10s} | LR  acc={acc_lr:.3f} f1={f1_lr:.3f} | MLP acc={acc_mlp:.3f} f1={f1_mlp:.3f}\")\n    return {\"Model\":name, \"LR_acc\":acc_lr, \"LR_f1\":f1_lr, \"MLP_acc\":acc_mlp, \"MLP_f1\":f1_mlp}\n\nrows = []\nrows.append(eval_block(Xv, Yv, Gv, \"V\"))\nrows.append(eval_block(Xa, Ya, Ga, \"A\"))\nrows.append(eval_block(Xt, Yt, Gt, \"T\"))\nrows.append(eval_block(Xva, Yva, Gva, \"V+A\"))\nrows.append(eval_block(Xvt, Yvt, Gvt, \"V+T\"))\nrows.append(eval_block(Xat, Yat, Gat, \"A+T\"))\nrows.append(eval_block(Xvat, Yvat, Gvat, \"V+A+T\"))\n\nres_df = pd.DataFrame(rows).sort_values(\"MLP_f1\", ascending=False)\nOUT_SUM = PROC_DIR / \"baseline_summary_VAT.csv\"\nres_df.to_csv(OUT_SUM, index=False)\nprint(\"Saved:\", OUT_SUM)\nres_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T19:47:04.107138Z","iopub.execute_input":"2025-09-20T19:47:04.107328Z","iopub.status.idle":"2025-09-20T20:04:44.233929Z","shell.execute_reply.started":"2025-09-20T19:47:04.107312Z","shell.execute_reply":"2025-09-20T20:04:44.233019Z"}},"outputs":[{"name":"stdout","text":"V          | LR  acc=0.716 f1=0.705 | MLP acc=0.717 f1=0.702\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"A          | LR  acc=0.632 f1=0.626 | MLP acc=0.576 f1=0.467\nT          | LR  acc=0.760 f1=0.752 | MLP acc=0.775 f1=0.763\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"V+A        | LR  acc=0.735 f1=0.726 | MLP acc=0.707 f1=0.681\nV+T        | LR  acc=0.786 f1=0.777 | MLP acc=0.801 f1=0.792\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"A+T        | LR  acc=0.779 f1=0.773 | MLP acc=0.776 f1=0.771\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"V+A+T      | LR  acc=0.797 f1=0.791 | MLP acc=0.798 f1=0.786\nSaved: /kaggle/working/baseline_summary_VAT.csv\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"   Model    LR_acc     LR_f1   MLP_acc    MLP_f1\n4    V+T  0.785714  0.776697  0.800952  0.792410\n6  V+A+T  0.797143  0.791347  0.798095  0.785755\n5    A+T  0.778517  0.772506  0.775665  0.770621\n2      T  0.759506  0.752169  0.774715  0.762883\n0      V  0.716004  0.705085  0.716929  0.702123\n3    V+A  0.734522  0.726460  0.707317  0.681407\n1      A  0.632022  0.625866  0.575843  0.466786","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>LR_acc</th>\n      <th>LR_f1</th>\n      <th>MLP_acc</th>\n      <th>MLP_f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>V+T</td>\n      <td>0.785714</td>\n      <td>0.776697</td>\n      <td>0.800952</td>\n      <td>0.792410</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>V+A+T</td>\n      <td>0.797143</td>\n      <td>0.791347</td>\n      <td>0.798095</td>\n      <td>0.785755</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>A+T</td>\n      <td>0.778517</td>\n      <td>0.772506</td>\n      <td>0.775665</td>\n      <td>0.770621</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>T</td>\n      <td>0.759506</td>\n      <td>0.752169</td>\n      <td>0.774715</td>\n      <td>0.762883</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>V</td>\n      <td>0.716004</td>\n      <td>0.705085</td>\n      <td>0.716929</td>\n      <td>0.702123</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>V+A</td>\n      <td>0.734522</td>\n      <td>0.726460</td>\n      <td>0.707317</td>\n      <td>0.681407</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A</td>\n      <td>0.632022</td>\n      <td>0.625866</td>\n      <td>0.575843</td>\n      <td>0.466786</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"import torch, torch.nn as nn\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass AttnFusion(nn.Module):\n    \"\"\"\n    Nhận tối đa 3 token (V, A, T) với chiều d_v, d_a, d_t.\n    Project -> d_model, +type embedding, TransformerEncoder, mean-pool, classifier.\n    \"\"\"\n    def __init__(self, dims, n_classes, d_model=256, nhead=8, nlayers=2, dropout=0.1):\n        super().__init__()\n        self.proj = nn.ModuleList([nn.Linear(d, d_model) for d in dims])  # one per modality\n        self.type_embed = nn.Embedding(len(dims), d_model)\n        enc_lyr = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=d_model*4, dropout=dropout, batch_first=True)\n        self.encoder = nn.TransformerEncoder(enc_lyr, num_layers=nlayers)\n        self.norm = nn.LayerNorm(d_model)\n        self.head = nn.Sequential(nn.Linear(d_model, d_model), nn.ReLU(), nn.Dropout(dropout), nn.Linear(d_model, n_classes))\n\n    def forward(self, xs):  # list of (B, d_k)\n        B = xs[0].size(0)\n        tokens = []\n        for i, x in enumerate(xs):\n            tokens.append(self.proj[i](x) + self.type_embed.weight[i])  # (B, d_model)\n        x = torch.stack(tokens, dim=1)  # (B, K, d_model)\n        x = self.encoder(x)\n        x = self.norm(x.mean(dim=1))\n        return self.head(x)\n\ndef attn_gkf(Xs, Y, G, epochs=12, lr=1e-3, batch=128):\n    gkf = GroupKFold(n_splits=min(5, len(np.unique(G))))\n    all_true, all_pred = [], []\n    for tr, te in gkf.split(Xs[0], Y, G):\n        # build tensors\n        Xtr = [torch.from_numpy(x[tr]).to(device) for x in Xs]\n        Xte = [torch.from_numpy(x[te]).to(device) for x in Xs]\n        ytr = torch.from_numpy(Y[tr]).to(device)\n        yte = Y[te]\n\n        model = AttnFusion([x.shape[1] for x in Xtr], n_classes=len(np.unique(Y))).to(device)\n        opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n        crit = nn.CrossEntropyLoss()\n\n        model.train()\n        for ep in range(epochs):\n            idx = np.random.permutation(len(tr))\n            for i in range(0, len(tr), batch):\n                sel = idx[i:i+batch]\n                batch_x = [x[sel] for x in Xtr]\n                batch_y = ytr[sel]\n                opt.zero_grad()\n                logits = model(batch_x)\n                loss = crit(logits, batch_y)\n                loss.backward()\n                opt.step()\n\n        model.eval()\n        with torch.no_grad():\n            logits = model(Xte).cpu().numpy()\n        pred = logits.argmax(axis=1)\n        all_true.append(yte); all_pred.append(pred)\n\n    all_true = np.concatenate(all_true); all_pred = np.concatenate(all_pred)\n    acc = accuracy_score(all_true, all_pred)\n    f1m = f1_score(all_true, all_pred, average=\"macro\")\n    return acc, f1m\n\n# Chạy cho V+A+T nếu khả dụng, nếu không thì chạy V+A\nif Xvat.size:\n    dims = [Xv.shape[1], Xa.shape[1], Xt.shape[1]]\n    # Cần đảm bảo những ma trận này align hàng — nếu bạn build bằng collect((\"v\",\"a\",\"t\")) thì đã align.\n    Xs = []\n    # tái build đúng hàng cho V,A,T dựa trên collect((\"v\",\"a\",\"t\")):\n    # ở đây Xvat là concat [V|A|T], nên tách ra:\n    dv, da, dt = dims\n    Xs = [Xvat[:, :dv], Xvat[:, dv:dv+da], Xvat[:, dv+da:dv+da+dt]]\n    acc, f1m = attn_gkf(Xs, Yvat, Gvat, epochs=12, lr=1e-3, batch=128)\n    print(f\"[Attn V+A+T] acc={acc:.3f} f1={f1m:.3f}\")\nelif Xva.size:\n    dv, da = Xv.shape[1], Xa.shape[1]\n    Xs = [Xva[:, :dv], Xva[:, dv:dv+da]]\n    acc, f1m = attn_gkf(Xs, Yva, Gva, epochs=12, lr=1e-3, batch=128)\n    print(f\"[Attn V+A] acc={acc:.3f} f1={f1m:.3f}\")\nelse:\n    print(\"No multimodal matrices available for attention fusion.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T20:04:44.234995Z","iopub.execute_input":"2025-09-20T20:04:44.235272Z","iopub.status.idle":"2025-09-20T20:04:47.456740Z","shell.execute_reply.started":"2025-09-20T20:04:44.235253Z","shell.execute_reply":"2025-09-20T20:04:47.456023Z"}},"outputs":[{"name":"stdout","text":"[Attn V+A+T] acc=0.599 f1=0.484\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
