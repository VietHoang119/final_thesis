{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13044847,"sourceType":"datasetVersion","datasetId":8260282},{"sourceId":13044890,"sourceType":"datasetVersion","datasetId":8188348}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"2749441d","cell_type":"markdown","source":"\n# Thesis Chapters 4–5 — Data Processing Notebook\n**Datasets**: MultiHateClip, HateClipSeg  \n**Environment**: Kaggle\n\nThis notebook standardizes the three datasets into CSV manifests that point to the **actual video paths** on Kaggle.\n\n**Input root (read-only):** `/kaggle/input/dataset-sample`  \n**Output root (writable):** `/kaggle/working/processed`\n\n","metadata":{}},{"id":"508af487-5179-4a59-a018-d3d481b36696","cell_type":"code","source":"from pathlib import Path\nimport pandas as pd\nimport json, ast\n\nDATASET_ROOT = Path(\"/kaggle/input/dataset-sample/preprocess_lab/data/raw\")\nOUT_DIR = Path(\"/kaggle/working/processed\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nCANON_LABELS = [\"normal\",\"offensive\",\"hateful\",\"insulting\",\"sexual\",\"violence\",\"self-harm\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T16:31:42.443512Z","iopub.execute_input":"2025-09-17T16:31:42.443891Z","iopub.status.idle":"2025-09-17T16:31:42.449817Z","shell.execute_reply.started":"2025-09-17T16:31:42.443865Z","shell.execute_reply":"2025-09-17T16:31:42.448783Z"}},"outputs":[],"execution_count":2},{"id":"22dbad73-2dd5-4a02-9c1e-3d1d7ad04f44","cell_type":"code","source":"# === Helpers: fuzzy column match, safe parsing, label normalization, utilities ===\n\ndef _norm_key(s: str) -> str:\n    \"\"\"Normalize string key: case/space/_/- insensitive.\"\"\"\n    if not isinstance(s, str):\n        s = str(s)\n    return s.strip().lower().replace(\" \", \"\").replace(\"_\",\"\").replace(\"-\",\"\")\n\ndef find_col(df: pd.DataFrame, *candidates):\n    \"\"\"Return the first matching column from candidates (fuzzy).\"\"\"\n    norm_map = {_norm_key(c): c for c in df.columns}\n    for cand in candidates:\n        key = _norm_key(cand)\n        if key in norm_map:\n            return norm_map[key]\n    return None\n\ndef safe_parse_list(x):\n    \"\"\"Parse list-like strings: try JSON then ast.literal_eval; return [] on failure.\"\"\"\n    if isinstance(x, (list, tuple)):\n        return list(x)\n    if not isinstance(x, str):\n        return []\n    s = x.strip()\n    try:\n        return json.loads(s)\n    except Exception:\n        try:\n            return ast.literal_eval(s)\n        except Exception:\n            return []\n\ndef normalize_label_string(x: str) -> str:\n    \"\"\"Map raw labels to canonical labels.\"\"\"\n    if x is None:\n        return \"normal\"\n    k = str(x).strip().lower()\n    m = {\n        \"nonhate\": \"normal\", \"non hate\": \"normal\", \"non_hate\": \"normal\", \"non-hate\": \"normal\",\n        \"none\": \"normal\", \"neutral\": \"normal\", \"benign\": \"normal\", \"normal\": \"normal\",\n        \"offensive\": \"offensive\",\n        \"hate\": \"hateful\", \"hateful\": \"hateful\",\n        \"insult\": \"insulting\", \"insulting\": \"insulting\",\n        \"sexual\": \"sexual\",\n        \"violence\": \"violence\", \"violent\": \"violence\",\n        \"harm\": \"self-harm\", \"self-harm\": \"self-harm\", \"self_harm\": \"self-harm\"\n    }\n    return m.get(k, k)\n\ndef onehot_to_labels(vec):\n    \"\"\"\n    Convert a multi-hot vector (list of 0/1) using HateClipSeg index mapping to canonical labels.\n    Mapping: 0=normal, 1=hateful, 2=insulting, 3=sexual, 4=violence, 5=self-harm\n    \"\"\"\n    idx2lab = [\"normal\", \"hateful\", \"insulting\", \"sexual\", \"violence\", \"self-harm\"]\n    out = []\n    for i, v in enumerate(vec):\n        try:\n            active = int(v) == 1\n        except Exception:\n            active = v == 1\n        if active and i < len(idx2lab):\n            out.append(idx2lab[i])\n    # Normalize + dedupe\n    seen, uniq = set(), []\n    for l in out:\n        l2 = normalize_label_string(l)\n        if l2 not in seen:\n            seen.add(l2)\n            uniq.append(l2)\n    return uniq\n\ndef any_offensive(labels_list):\n    \"\"\"Return 1 if any non-'normal' label appears; else 0.\"\"\"\n    return int(any(normalize_label_string(l) != \"normal\" for l in labels_list))\n\ndef explode_single_label_rows(df, labels_col=\"labels\", keep_cols=None):\n    \"\"\"\n    Given a dataframe where `labels` is a list, explode into single-label rows.\n    keep_cols: columns to keep as-is (copied to each exploded row).\n    \"\"\"\n    if keep_cols is None:\n        keep_cols = [c for c in df.columns if c != labels_col]\n    rows = []\n    for _, r in df.iterrows():\n        labs = r[labels_col] if isinstance(r[labels_col], list) else safe_parse_list(r[labels_col])\n        if not labs:\n            labs = [\"normal\"]\n        for lb in labs:\n            newr = {c: r[c] for c in keep_cols}\n            newr[\"label\"] = normalize_label_string(lb)  # single canonical label\n            rows.append(newr)\n    return pd.DataFrame(rows)\n\ndef build_sample_id(dataset, video_id, start, end, label):\n    \"\"\"Build a deterministic sample_id for downstream training/caching.\"\"\"\n    s = \"0\" if (start is None or pd.isna(start)) else f\"{float(start):.3f}\"\n    e = \"end\" if (end is None or pd.isna(end)) else f\"{float(end):.3f}\"\n    return f\"{dataset}__{video_id}__{s}_{e}__{label}\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T16:34:55.519081Z","iopub.execute_input":"2025-09-17T16:34:55.519466Z","iopub.status.idle":"2025-09-17T16:34:55.541255Z","shell.execute_reply.started":"2025-09-17T16:34:55.519439Z","shell.execute_reply":"2025-09-17T16:34:55.539741Z"}},"outputs":[],"execution_count":8},{"id":"f163d106-fe3c-428f-8b3b-36374b6074b8","cell_type":"code","source":"# --- MultiHateClip ---\nmh_root = DATASET_ROOT / \"MultiHateClip\"\nmh_train = pd.read_csv(mh_root / \"train.tsv\", sep=\"\\t\")\nmh_valid = pd.read_csv(mh_root / \"valid.tsv\", sep=\"\\t\")\nmh_test  = pd.read_csv(mh_root / \"test.tsv\",  sep=\"\\t\")\ndf_mh = pd.concat([mh_train,mh_valid,mh_test],keys=[\"train\",\"valid\",\"test\"],names=[\"split\"]).reset_index(level=0)\nvid_col = \"Video_ID\"\nlbl_col = \"Label\"\n\nrecords = []\nfor _,r in df_mh.iterrows():\n    vid = str(r[vid_col]).strip()\n    lab = normalize_label_string(r[lbl_col])\n    records.append({\n        \"dataset\":\"MultiHateClip\",\"Video_ID\":vid,\n        \"video_path\": str(mh_root/\"data\"/r[\"split\"]/f\"{vid}.mp4\"),\n        \"labels\":[lab],\"binary_offensive\":int(lab!=\"normal\"),\n        \"start_sec\":None,\"end_sec\":None\n    })\nmh_df = pd.DataFrame(records)\nmh_single = explode_single_label_rows(mh_df,\"labels\",\n    [\"dataset\",\"Video_ID\",\"video_path\",\"binary_offensive\",\"start_sec\",\"end_sec\"])\nmh_single[\"sample_id\"] = mh_single.apply(lambda r: build_sample_id(r.dataset,r.Video_ID,r.start_sec,r.end_sec,r.label),axis=1)\nmh_single.to_csv(OUT_DIR/\"multihateclip_samples.csv\",index=False)\n\n# --- HateClipSeg ---\nhcs_root = DATASET_ROOT / \"HateClipSeg\"\ndf_seg = pd.read_csv(hcs_root/\"segment_level_annotation.csv\")\ncol_vid, col_lbl, col_ts = \"Video Id\",\"Segment-Level Label\",\"Segment Timestamp\"\n\nrecords=[]\nfor _,r in df_seg.iterrows():\n    vid=str(r[col_vid]).strip()\n    labels= safe_parse_list(r[col_lbl])\n    times = safe_parse_list(r[col_ts])\n    for lb,ts in zip(labels,times):\n        labs = onehot_to_labels(lb)\n        t0,t1 = float(ts[0]), float(ts[1])\n        vpath = hcs_root/\"data\"/\"segment_level\"/f\"{vid}.mp4\"\n        records.append({\n            \"dataset\":\"HateClipSeg\",\"Video_ID\":vid,\"video_path\":str(vpath),\n            \"labels\":labs,\"binary_offensive\":any(l!=\"normal\" for l in labs),\n            \"start_sec\":t0,\"end_sec\":t1\n        })\nhcs_df = pd.DataFrame(records)\nhcs_single = explode_single_label_rows(hcs_df,\"labels\",\n    [\"dataset\",\"Video_ID\",\"video_path\",\"binary_offensive\",\"start_sec\",\"end_sec\"])\nhcs_single[\"sample_id\"] = hcs_single.apply(lambda r: build_sample_id(r.dataset,r.Video_ID,r.start_sec,r.end_sec,r.label),axis=1)\nhcs_single.to_csv(OUT_DIR/\"hateclipseg_samples.csv\",index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T16:35:46.843301Z","iopub.execute_input":"2025-09-17T16:35:46.843993Z","iopub.status.idle":"2025-09-17T16:35:48.737942Z","shell.execute_reply.started":"2025-09-17T16:35:46.843965Z","shell.execute_reply":"2025-09-17T16:35:48.736886Z"}},"outputs":[],"execution_count":11},{"id":"1b445752-c073-48c1-937e-7d51418aaddf","cell_type":"code","source":"parts=[pd.read_csv(OUT_DIR/\"multihateclip_samples.csv\"),\n       pd.read_csv(OUT_DIR/\"hateclipseg_samples.csv\")]\nunified=pd.concat(parts,ignore_index=True)\nunified.to_csv(OUT_DIR/\"unified_samples.csv\",index=False)\nprint(\"Unified saved:\",len(unified))\nprint(unified.label.value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T16:36:37.677175Z","iopub.execute_input":"2025-09-17T16:36:37.677460Z","iopub.status.idle":"2025-09-17T16:36:37.887190Z","shell.execute_reply.started":"2025-09-17T16:36:37.677439Z","shell.execute_reply":"2025-09-17T16:36:37.886067Z"}},"outputs":[{"name":"stdout","text":"Unified saved: 14467\nlabel\nnormal                                             6491\ninsulting                                          2920\nhateful                                            2363\nviolence                                           1281\n['normal', 'normal']                                585\nsexual                                              372\n['offensive', 'offensive']                          174\nself-harm                                            39\n['counter narrative', 'normal']                      30\n['hateful', 'hateful']                               29\n['normal', 'offensive', 'offensive']                 27\n['offensive', 'normal', 'normal']                    22\n['hateful', 'offensive', 'hateful']                  18\n['offensive', 'normal', 'offensive']                 15\n['offensive', 'hateful', 'hateful']                  15\n['hateful', 'offensive', 'offensive']                12\n['counter narrative', 'offensive', 'offensive']      11\n['normal', 'offensive', 'normal']                     9\n['offensive', 'hateful', 'offensive']                 9\n['offensive', 'counter narrative', 'offensive']       8\n['normal', 'counter narrative']                       6\n['hateful', 'normal', 'normal']                       6\n['normal', 'hateful', 'hateful']                      6\n['hateful', 'normal', 'hateful']                      5\n['hateful', 'counter narrative', 'hateful']           5\n['normal', 'hateful', 'normal']                       3\n['counter narrative', 'hateful', 'hateful']           2\n['offensive', 'normal', 'hateful', 'hateful']         1\n['hateful', 'offensive', 'normal', 'hateful']         1\n['hateful', 'normal', 'offensive', 'offensive']       1\n['normal', 'hateful', 'counter narrative']            1\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":12},{"id":"28f5cb09-9637-4d77-9a51-4c01d244b883","cell_type":"code","source":"from pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport subprocess, shlex, math\n\n# Input manifests (generated earlier)\nOUT_DIR = Path(\"/kaggle/working/processed\")\nMHC_IN  = OUT_DIR / \"multihateclip_samples.csv\"\nHCS_IN  = OUT_DIR / \"hateclipseg_samples.csv\"\n\nassert MHC_IN.exists(), f\"Missing {MHC_IN}\"\nassert HCS_IN.exists(), f\"Missing {HCS_IN}\"\n\n# Outputs\nMHC_CLEAN = OUT_DIR / \"multihateclip_samples.cleaned.csv\"\nHCS_CLEAN = OUT_DIR / \"hateclipseg_samples.cleaned.csv\"\nUNIFIED_CLEAN = OUT_DIR / \"unified_samples.cleaned.csv\"\n\n# Optional: repair broken MP4 into /kaggle/working/repair\nREPAIR_BROKEN_MP4 = False\nREPAIR_DIR = Path(\"/kaggle/working/repair\")\nif REPAIR_BROKEN_MP4:\n    REPAIR_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(\"Repair broken mp4?\", REPAIR_BROKEN_MP4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:15:58.883134Z","iopub.execute_input":"2025-09-17T17:15:58.883615Z","iopub.status.idle":"2025-09-17T17:15:58.892333Z","shell.execute_reply.started":"2025-09-17T17:15:58.883552Z","shell.execute_reply":"2025-09-17T17:15:58.890849Z"}},"outputs":[{"name":"stdout","text":"Repair broken mp4? False\n","output_type":"stream"}],"execution_count":30},{"id":"ff450439-fbc6-49ae-957a-eecfc6d9a517","cell_type":"code","source":"# Robust duration probing (ffprobe first, OpenCV fallback) + optional repair via remux\ntry:\n    import cv2\nexcept Exception:\n    cv2 = None\n\ndef _exists_and_nonempty(p: str | Path) -> bool:\n    try:\n        p = Path(p)\n        return p.exists() and p.is_file() and p.stat().st_size > 0\n    except Exception:\n        return False\n\ndef _ffprobe_duration(p: str) -> float | None:\n    cmd = (\n        \"ffprobe -v error -analyzeduration 100M -probesize 100M \"\n        \"-select_streams v:0 -show_entries format=duration \"\n        \"-of default=nw=1:nk=1 \"\n        f\"{shlex.quote(str(p))}\"\n    )\n    try:\n        out = subprocess.check_output(shlex.split(cmd), stderr=subprocess.DEVNULL, text=True).strip()\n        dur = float(out)\n        if math.isfinite(dur) and dur > 0:\n            return dur\n    except Exception:\n        return None\n    return None\n\ndef _opencv_duration(p: str) -> float | None:\n    if cv2 is None:\n        return None\n    try:\n        cap = cv2.VideoCapture(str(p))\n        if not cap.isOpened():\n            return None\n        fps = cap.get(cv2.CAP_PROP_FPS) or 0.0\n        frames = cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0.0\n        cap.release()\n        if fps > 0 and frames > 0:\n            return float(frames / fps)\n    except Exception:\n        return None\n    return None\n\ndef _try_repair_mp4(src: str, repair_dir: Path) -> str | None:\n    \"\"\"\n    Remux MP4 to generate missing moov atom; fallback to re-encode if remux fails.\n    Returns repaired path or None.\n    \"\"\"\n    try:\n        srcp = Path(src)\n        dst = repair_dir / f\"{srcp.stem}_repaired.mp4\"\n        # Fast remux\n        cmd = (\n            f'ffmpeg -hide_banner -loglevel error -y '\n            f'-err_detect ignore_err -fflags +genpts -i {shlex.quote(src)} '\n            f'-c copy -movflags +faststart {shlex.quote(str(dst))}'\n        )\n        ok = subprocess.run(shlex.split(cmd)).returncode == 0\n        if not ok:\n            # Re-encode fallback\n            cmd = (\n                f'ffmpeg -hide_banner -loglevel error -y '\n                f'-err_detect ignore_err -fflags +genpts -i {shlex.quote(src)} '\n                f'-c:v libx264 -c:a aac -movflags +faststart {shlex.quote(str(dst))}'\n            )\n            ok = subprocess.run(shlex.split(cmd)).returncode == 0\n        if ok:\n            dur = _ffprobe_duration(str(dst)) or _opencv_duration(str(dst))\n            if dur and dur > 0:\n                return str(dst)\n    except Exception:\n        return None\n    return None\n\ndef probe_or_repair(path: str, allow_repair: bool = False) -> tuple[float|None, str|None, bool]:\n    \"\"\"\n    Returns: (duration_seconds, playable_path, repaired_flag)\n    - playable_path is original or repaired; None if unusable.\n    - repaired_flag True if we generated and used a repaired file.\n    \"\"\"\n    if not _exists_and_nonempty(path):\n        return None, None, False\n    dur = _ffprobe_duration(path)\n    if dur and dur > 0:\n        return dur, path, False\n    # Try OpenCV fallback\n    dur_cv = _opencv_duration(path)\n    if dur_cv and dur_cv > 0:\n        return dur_cv, path, False\n    # Optional repair\n    if allow_repair:\n        repaired = _try_repair_mp4(path, REPAIR_DIR)\n        if repaired:\n            dur2 = _ffprobe_duration(repaired) or _opencv_duration(repaired)\n            if dur2 and dur2 > 0:\n                return dur2, repaired, True\n    # Unusable\n    return None, None, False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:16:12.555954Z","iopub.execute_input":"2025-09-17T17:16:12.556400Z","iopub.status.idle":"2025-09-17T17:16:12.572498Z","shell.execute_reply.started":"2025-09-17T17:16:12.556371Z","shell.execute_reply":"2025-09-17T17:16:12.570450Z"}},"outputs":[],"execution_count":31},{"id":"d86b92bc-8f28-40de-9eb5-a7e9e466eba3","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport subprocess, shlex, math\nfrom pathlib import Path\n\n# ---------- Video probing helpers ----------\ndef _ffprobe_duration(path: str) -> float | None:\n    \"\"\"Return video duration in seconds using ffprobe.\"\"\"\n    cmd = f\"ffprobe -v error -show_entries format=duration -of default=nw=1:nk=1 {shlex.quote(str(path))}\"\n    try:\n        out = subprocess.check_output(shlex.split(cmd), stderr=subprocess.DEVNULL, text=True).strip()\n        dur = float(out)\n        if math.isfinite(dur) and dur > 0:\n            return dur\n    except Exception:\n        return None\n    return None\n\ndef probe_duration(path: str) -> float | None:\n    \"\"\"Wrapper to probe duration; returns None if file is broken/unreadable.\"\"\"\n    try:\n        return _ffprobe_duration(path)\n    except Exception:\n        return None\n\n# ---------- Manifest cleaning ----------\ndef clean_and_stats(manifest_csv: str, dataset_name: str):\n    \"\"\"\n    - Load manifest (columns: sample_id, video_path, label, start_sec, end_sec, ...)\n    - Probe each unique video_path\n    - Drop broken/unreadable files\n    - Return cleaned df and stats summary\n    \"\"\"\n    df = pd.read_csv(manifest_csv)\n    if \"video_path\" not in df.columns:\n        raise ValueError(f\"{manifest_csv} missing 'video_path'\")\n\n    # Probe unique videos\n    uniq = df[\"video_path\"].dropna().unique().tolist()\n    results = {}\n    for p in uniq:\n        dur = probe_duration(p)\n        results[p] = dur\n\n    df[\"video_duration\"] = df[\"video_path\"].map(results)\n\n    # segment length if start/end available\n    def seg_len(row):\n        try:\n            if pd.notna(row.get(\"start_sec\")) and pd.notna(row.get(\"end_sec\")):\n                return max(0.0, float(row[\"end_sec\"]) - float(row[\"start_sec\"]))\n        except Exception:\n            pass\n        return np.nan\n    df[\"segment_seconds\"] = df.apply(seg_len, axis=1)\n\n    # Mark broken\n    df[\"is_broken\"] = df[\"video_duration\"].isna()\n\n    total = len(df)\n    broken = df[\"is_broken\"].sum()\n    kept = total - broken\n\n    # Stats per label\n    stats = df[~df[\"is_broken\"]].groupby(\"label\").agg(\n        samples=(\"label\", \"count\"),\n        avg_video_len=(\"video_duration\", \"mean\"),\n        avg_seg_len=(\"segment_seconds\", \"mean\")\n    ).reset_index()\n\n    print(f\"\\n=== {dataset_name} ===\")\n    print(f\"Total samples: {total}\")\n    print(f\"Broken/unusable: {broken} ({broken/total:.2%})\")\n    print(f\"Usable: {kept}\")\n    print(\"\\nPer-class stats:\")\n    display(stats)\n\n    return df[~df[\"is_broken\"]], stats\n\n# ---------- Run on your manifests ----------\nmhc_clean, mhc_stats = clean_and_stats(\"/kaggle/working/processed/multihateclip_samples.csv\", \"MultiHateClip\")\nhcs_clean, hcs_stats = clean_and_stats(\"/kaggle/working/processed/hateclipseg_samples.csv\", \"HateClipSeg\")\n\n# Merge unified dataset\nunified = pd.concat([mhc_clean, hcs_clean], ignore_index=True)\nprint(\"\\n=== Unified Dataset ===\")\nprint(\"Total usable samples:\", len(unified))\nprint(\"Label distribution:\\n\", unified[\"label\"].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:18:07.611915Z","iopub.execute_input":"2025-09-17T17:18:07.612530Z","iopub.status.idle":"2025-09-17T17:21:06.648621Z","shell.execute_reply.started":"2025-09-17T17:18:07.612503Z","shell.execute_reply":"2025-09-17T17:21:06.647558Z"}},"outputs":[{"name":"stdout","text":"\n=== MultiHateClip ===\nTotal samples: 1001\nBroken/unusable: 998 (99.70%)\nUsable: 3\n\nPer-class stats:\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                             label  samples  avg_video_len  avg_seg_len\n0  ['counter narrative', 'normal']        1      57.144271          NaN\n1             ['normal', 'normal']        2      31.857774          NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>samples</th>\n      <th>avg_video_len</th>\n      <th>avg_seg_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>['counter narrative', 'normal']</td>\n      <td>1</td>\n      <td>57.144271</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>['normal', 'normal']</td>\n      <td>2</td>\n      <td>31.857774</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\n=== HateClipSeg ===\nTotal samples: 13466\nBroken/unusable: 13466 (100.00%)\nUsable: 0\n\nPer-class stats:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Empty DataFrame\nColumns: [label, samples, avg_video_len, avg_seg_len]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>samples</th>\n      <th>avg_video_len</th>\n      <th>avg_seg_len</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\n=== Unified Dataset ===\nTotal usable samples: 3\nLabel distribution:\n label\n['normal', 'normal']               2\n['counter narrative', 'normal']    1\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3206496420.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  unified = pd.concat([mhc_clean, hcs_clean], ignore_index=True)\n","output_type":"stream"}],"execution_count":35},{"id":"67b46fe0-4efa-494a-af29-1599e7fc4782","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
